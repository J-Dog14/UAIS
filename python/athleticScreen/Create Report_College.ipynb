{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-07T21:07:25.663564Z",
     "start_time": "2025-11-07T21:07:23.926766Z"
    }
   },
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import re\n",
    "from math import inf\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "folder_path = r'D:/Athletic Screen 2.0/Output Files/'\n",
    "db_path     = r'D:/Athletic Screen 2.0/Output Files/movement_database_v2.db'\n",
    "\n",
    "# fresh db\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "    print(f\"Deleted existing database at {db_path}\")\n",
    "\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "conn.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "conn.execute(\"PRAGMA busy_timeout=5000;\") \n",
    "\n",
    "\n",
    "# --- schemas (unchanged from your message) ---\n",
    "table_schemas = {\n",
    "    'CMJ': '''CREATE TABLE IF NOT EXISTS CMJ (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                name TEXT,\n",
    "                date TEXT,\n",
    "                trial_name TEXT,\n",
    "                JH_IN REAL,\n",
    "                Peak_Power REAL,\n",
    "                PP_FORCEPLATE REAL,\n",
    "                Force_at_PP REAL,\n",
    "                Vel_at_PP REAL,\n",
    "                PP_W_per_kg REAL\n",
    "              )''',\n",
    "    'PPU': '''CREATE TABLE IF NOT EXISTS PPU (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                name TEXT,\n",
    "                date TEXT,\n",
    "                trial_name TEXT,\n",
    "                JH_IN REAL,\n",
    "                Peak_Power REAL,\n",
    "                PP_FORCEPLATE REAL,\n",
    "                Force_at_PP REAL,\n",
    "                Vel_at_PP REAL,\n",
    "                PP_W_per_kg REAL\n",
    "            )''',\n",
    "    'DJ':  '''CREATE TABLE IF NOT EXISTS DJ (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                name TEXT,\n",
    "                date TEXT,\n",
    "                trial_name TEXT,\n",
    "                JH_IN REAL,\n",
    "                PP_FORCEPLATE REAL,\n",
    "                Force_at_PP REAL,\n",
    "                Vel_at_PP REAL,\n",
    "                PP_W_per_kg REAL,\n",
    "                CT REAL,\n",
    "                RSI REAL\n",
    "              )''',\n",
    "    'SLV': '''CREATE TABLE IF NOT EXISTS SLV (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                name TEXT,\n",
    "                date TEXT, \n",
    "                trial_name TEXT,\n",
    "                side TEXT,\n",
    "                JH_IN REAL,\n",
    "                PP_FORCEPLATE REAL,\n",
    "                Force_at_PP REAL,\n",
    "                Vel_at_PP REAL,\n",
    "                PP_W_per_kg REAL\n",
    "              )''',\n",
    "    'NMT': '''CREATE TABLE IF NOT EXISTS NMT (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                name TEXT,\n",
    "                date TEXT, \n",
    "                trial_name TEXT,\n",
    "                NUM_TAPS_10s REAL,\n",
    "                NUM_TAPS_20s REAL,\n",
    "                NUM_TAPS_30s REAL,\n",
    "                NUM_TAPS REAL\n",
    "              )'''\n",
    "}\n",
    "\n",
    "for schema in table_schemas.values():\n",
    "    cursor.execute(schema)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def extract_name(line):\n",
    "    m = re.search(r'Data\\\\(.*?)[_\\\\]', line)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def extract_date(line):\n",
    "    m = re.search(r'\\\\(\\d{4}-\\d{2}-\\d{2})_', line)\n",
    "    return m.group(1) if m else None\n",
    "\n",
    "def read_first_numeric_row_values(fobj):\n",
    "    \"\"\"Return list of floats from the first numeric line encountered.\"\"\"\n",
    "    for line in fobj:\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if re.match(r'^[-+]?\\d', line):   # numeric line\n",
    "            return [float(tok) for tok in line.split()]\n",
    "    return []\n",
    "\n",
    "def peak_power_from_pow_file(trial_name_base):\n",
    "    \"\"\"\n",
    "    Look for '{trial_name_base}_Power.txt' in folder_path, parse PowZ time series,\n",
    "    and return its max (or None if not found/empty).\n",
    "    \"\"\"\n",
    "    power_file = os.path.join(folder_path, f\"{trial_name_base}_Power.txt\")\n",
    "    if not os.path.exists(power_file):\n",
    "        return None\n",
    "\n",
    "    peak = -inf\n",
    "    try:\n",
    "        with open(power_file, 'r') as pf:\n",
    "            # Skip header lines until numeric rows start (format: ITEM <tab> X)\n",
    "            for line in pf:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                if re.match(r'^\\d+\\s+', line):  # rows like \"1\\t0.00000\"\n",
    "                    # from now on, all lines *should* be numeric rows\n",
    "                    # take the second column as PowZ value\n",
    "                    parts = line.split()\n",
    "                    if len(parts) >= 2:\n",
    "                        val = float(parts[1])\n",
    "                        if val > peak:\n",
    "                            peak = val\n",
    "                    # continue through rest of file\n",
    "                    break\n",
    "            # continue consuming rest of numeric rows\n",
    "            for line in pf:\n",
    "                line = line.strip()\n",
    "                if not line or not re.match(r'^\\d+\\s+', line):\n",
    "                    continue\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    val = float(parts[1])\n",
    "                    if val > peak:\n",
    "                        peak = val\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "    return None if peak == -inf else peak\n",
    "\n",
    "def insert_row(table, cols, vals):\n",
    "    placeholders = \",\".join([\"?\"] * len(vals))\n",
    "    cursor.execute(f\"INSERT INTO {table} ({','.join(cols)}) VALUES ({placeholders})\", vals)\n",
    "\n",
    "# ---------- main loop ----------\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if not file_name.endswith('.txt'):\n",
    "        continue\n",
    "    if file_name.endswith('_Power.txt'):\n",
    "        # handled separately when computing Peak_Power; skip in main loop\n",
    "        continue\n",
    "\n",
    "    trial_name = os.path.splitext(file_name)[0]\n",
    "    # classify\n",
    "    if 'CMJ' in trial_name:\n",
    "        table = 'CMJ'\n",
    "    elif 'PPU' in trial_name:\n",
    "        table = 'PPU'\n",
    "    elif 'DJ' in trial_name:\n",
    "        table = 'DJ'\n",
    "    elif 'SLVL' in trial_name or 'SLVR' in trial_name:\n",
    "        table = 'SLV'\n",
    "    elif 'NMT' in trial_name:\n",
    "        table = 'NMT'\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            first_line = f.readline().strip()\n",
    "            name = extract_name(first_line)\n",
    "            date = extract_date(first_line)\n",
    "            if not name:\n",
    "                print(f\"Name extraction failed for {file_name}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            variables = read_first_numeric_row_values(f)\n",
    "            if not variables:\n",
    "                print(f\"No numeric data found in {file_name}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing file: {file_name}, Variables: {variables}\")\n",
    "\n",
    "            # drop leading dummy \"1\"\n",
    "            v = variables[1:] if variables and variables[0] == 1.0 else variables[:]\n",
    "\n",
    "            # movement-specific handling\n",
    "            if table in ('CMJ', 'PPU'):\n",
    "                # expected 5 values: JH, PP_FORCEPLATE, Force@PP, Vel@PP, W/kg\n",
    "                # compute Peak_Power from corresponding *_Power.txt if available\n",
    "                peak_power = peak_power_from_pow_file(trial_name)\n",
    "                if len(v) == 5:\n",
    "                    JH, PP_FP, F_at_PP, V_at_PP, Wkg = v\n",
    "                elif len(v) == 6:\n",
    "                    # sometimes Peak_Power included as v[1]; keep robust fallback\n",
    "                    JH = v[0]\n",
    "                    if peak_power is None:\n",
    "                        peak_power = v[1]\n",
    "                    PP_FP, F_at_PP, V_at_PP, Wkg = v[-4:]\n",
    "                else:\n",
    "                    print(f\"Unexpected column count for {file_name}: {len(v)}; skipping.\")\n",
    "                    continue\n",
    "\n",
    "                cols = [\"name\",\"date\",\"trial_name\",\"JH_IN\",\"Peak_Power\",\"PP_FORCEPLATE\",\"Force_at_PP\",\"Vel_at_PP\",\"PP_W_per_kg\"]\n",
    "                vals = [name, date, trial_name, JH, peak_power, PP_FP, F_at_PP, V_at_PP, Wkg]\n",
    "                insert_row(table, cols, vals)\n",
    "    \n",
    "            elif table == 'DJ':\n",
    "                # v must be exactly 7 values in this order:\n",
    "                # [JH_IN, PP_FORCEPLATE, Force_at_PP, Vel_at_PP, CT, RSI, PP_W_per_kg]\n",
    "                if len(v) != 7:\n",
    "                    print(f\"Unexpected DJ column count for {file_name}: {len(v)}; expected 7. Skipping.\")\n",
    "                    continue\n",
    "            \n",
    "                JH, PP_FP, F_at_PP, V_at_PP, CT, RSI, Wkg = v  # do not modify values (keep negatives if present)\n",
    "            \n",
    "                cols = [\n",
    "                    \"name\",\"date\",\"trial_name\",\n",
    "                    \"JH_IN\",\"PP_FORCEPLATE\",\"Force_at_PP\",\"Vel_at_PP\",\n",
    "                    \"PP_W_per_kg\",\"CT\",\"RSI\"\n",
    "                ]\n",
    "                vals = [\n",
    "                    name, date, trial_name,\n",
    "                    JH, PP_FP, F_at_PP, V_at_PP,\n",
    "                    Wkg, CT, RSI\n",
    "                ]\n",
    "                insert_row(\"DJ\", cols, vals)\n",
    "\n",
    "            elif table == 'SLV':\n",
    "                side = 'Left' if 'SLVL' in trial_name else 'Right'\n",
    "                # SLV sometimes includes Peak_Power as the second item (making 6 vals total).\n",
    "                # Your SLV schema *does not* have Peak_Power; we'll ignore it if present.\n",
    "                if len(v) == 6:\n",
    "                    # v = [JH, Peak, PP_FP, F_at_PP, V_at_PP, Wkg] -> drop Peak\n",
    "                    JH, PP_FP, F_at_PP, V_at_PP, Wkg = v[0], v[2], v[3], v[4], v[5]\n",
    "                elif len(v) == 5:\n",
    "                    JH, PP_FP, F_at_PP, V_at_PP, Wkg = v\n",
    "                else:\n",
    "                    print(f\"Unexpected SLV column count for {file_name}: {len(v)}; skipping.\")\n",
    "                    continue\n",
    "\n",
    "                cols = [\"name\",\"date\",\"trial_name\",\"side\",\"JH_IN\",\"PP_FORCEPLATE\",\"Force_at_PP\",\"Vel_at_PP\",\"PP_W_per_kg\"]\n",
    "                vals = [name, date, trial_name, side, JH, PP_FP, F_at_PP, V_at_PP, Wkg]\n",
    "                insert_row(table, cols, vals)\n",
    "\n",
    "            elif table == 'NMT':\n",
    "                if len(v) != 4:\n",
    "                    print(f\"Unexpected NMT column count for {file_name}: {len(v)}; skipping.\")\n",
    "                    continue\n",
    "                cols = [\"name\",\"date\",\"trial_name\",\"NUM_TAPS_10s\",\"NUM_TAPS_20s\",\"NUM_TAPS_30s\",\"NUM_TAPS\"]\n",
    "                vals = [name, date, trial_name] + v\n",
    "                insert_row(table, cols, vals)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error with file {file_name}: {e}\")\n",
    "\n",
    "def load_power_txt(txt_path: str):\n",
    "    \"\"\"Read <trial>_Power.txt -> return 1-D numpy/pandas-like series of PowZ.\"\"\"\n",
    "    vals = []\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        # find first numeric row like: \"1\\t0.00000\"\n",
    "        in_numeric = False\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if not in_numeric and re.match(r'^\\d+\\s+', line):\n",
    "                in_numeric = True\n",
    "            if in_numeric and re.match(r'^\\d+\\s+', line):\n",
    "                parts = re.split(r'\\s+', line)\n",
    "                if len(parts) >= 2:\n",
    "                    try:\n",
    "                        vals.append(float(parts[1]))  # second column = power\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "    if not vals:\n",
    "        raise ValueError(f\"No numeric power values in {txt_path}\")\n",
    "    # return a simple list or numpy array is fine for our analysis funcs\n",
    "    return np.asarray(vals, dtype=float)\n",
    "\n",
    "def analyze_power_curve(power, fs_hz: float = 1000.0):\n",
    "    \"\"\"Base metrics used by analyze_power_curve_advanced.\"\"\"\n",
    "    p = np.asarray(power, dtype=float)\n",
    "    n = p.size\n",
    "    t = np.arange(n) / fs_hz\n",
    "\n",
    "    pk_idx = int(np.nanargmax(p))\n",
    "    pk_val = float(p[pk_idx])\n",
    "\n",
    "    thr10 = 0.10 * pk_val\n",
    "    thr50 = 0.50 * pk_val\n",
    "    thr90 = 0.90 * pk_val\n",
    "\n",
    "    try:\n",
    "        onset_idx = int(np.argmax(p >= thr10))\n",
    "    except ValueError:\n",
    "        onset_idx = 0\n",
    "    post = p[pk_idx:]\n",
    "    off_rel = np.argmax(post < thr10) if np.any(post < thr10) else (post.size - 1)\n",
    "    offset_idx = pk_idx + int(off_rel)\n",
    "\n",
    "    rising = p[:pk_idx+1]\n",
    "    try:\n",
    "        i10 = int(np.argmax(rising >= thr10))\n",
    "        i90 = int(np.argmax(rising >= thr90))\n",
    "        rise_time = (i90 - i10) / fs_hz if i90 > i10 else np.nan\n",
    "        rise_slope = (0.8 * pk_val) / rise_time if rise_time and rise_time > 0 else np.nan\n",
    "    except ValueError:\n",
    "        i10 = i90 = None\n",
    "        rise_time = np.nan\n",
    "        rise_slope = np.nan\n",
    "\n",
    "    try:\n",
    "        left_idx  = int(np.argmax(rising >= thr50))\n",
    "    except ValueError:\n",
    "        left_idx = pk_idx\n",
    "    falling = p[pk_idx:]\n",
    "    try:\n",
    "        right_rel = int(np.argmax(falling <= thr50))\n",
    "        right_idx = pk_idx + right_rel\n",
    "    except ValueError:\n",
    "        right_idx = pk_idx\n",
    "    fwhm_sec = (right_idx - left_idx) / fs_hz if right_idx > left_idx else np.nan\n",
    "\n",
    "    a = max(0, onset_idx)\n",
    "    b = min(n - 1, max(offset_idx, pk_idx))\n",
    "    auc_joules = float(np.trapezoid(np.nan_to_num(p[a:b+1], nan=0.0), dx=1.0/fs_hz))\n",
    "\n",
    "    weights = np.clip(p[a:b+1], a_min=0, a_max=None)\n",
    "    if np.sum(weights) > 0:\n",
    "        t_window = t[a:b+1]\n",
    "        t_com = float(np.sum(t_window * weights) / np.sum(weights))\n",
    "        t_com_norm = (t_com - t[a]) / max(1e-9, (t[b] - t[a]))\n",
    "    else:\n",
    "        t_com = np.nan\n",
    "        t_com_norm = np.nan\n",
    "\n",
    "    w = int(0.05 * fs_hz)\n",
    "    lo = max(0, pk_idx - w)\n",
    "    hi = min(n, pk_idx + w + 1)\n",
    "    local = p[lo:hi]\n",
    "    cv_local = float(np.std(local) / np.mean(local)) if np.mean(local) > 0 else np.nan\n",
    "\n",
    "    return {\n",
    "        \"n_samples\": n,\n",
    "        \"fs_hz\": fs_hz,\n",
    "        \"peak_power_w\": pk_val,\n",
    "        \"time_to_peak_s\": float(t[pk_idx]),\n",
    "        \"rise_time_10_90_s\": float(rise_time),\n",
    "        \"rise_slope_w_per_s\": float(rise_slope),\n",
    "        \"fwhm_s\": float(fwhm_sec),\n",
    "        \"auc_j\": auc_joules,\n",
    "        \"onset_idx\": a,\n",
    "        \"offset_idx\": b,\n",
    "        \"peak_idx\": pk_idx,\n",
    "        \"t_com_s\": t_com,\n",
    "        \"t_com_norm_0to1\": t_com_norm,\n",
    "        \"cv_local_peak\": cv_local,\n",
    "        \"i10_idx\": int(i10) if isinstance(i10, int) else None,\n",
    "        \"i90_idx\": int(i90) if isinstance(i90, int) else None,\n",
    "        \"left50_idx\": left_idx,\n",
    "        \"right50_idx\": right_idx,\n",
    "    }\n",
    "\n",
    "def analyze_power_curve_advanced(power, fs_hz: float = 1000.0):\n",
    "    \"\"\"Adds RPD, early/late work, decay, shape stats, spectral centroid.\"\"\"\n",
    "    base = analyze_power_curve(power, fs_hz)\n",
    "    p = np.asarray(power, dtype=float)\n",
    "    n = p.size\n",
    "\n",
    "    dp = np.gradient(p, 1.0/fs_hz)\n",
    "    base[\"rpd_max_w_per_s\"]   = float(np.nanmax(dp))\n",
    "    base[\"time_to_rpd_max_s\"] = float(np.nanargmax(dp) / fs_hz)\n",
    "\n",
    "    a, b, pk = base[\"onset_idx\"], base[\"offset_idx\"], base[\"peak_idx\"]\n",
    "    auc_pre  = float(np.trapezoid(np.nan_to_num(p[a:pk+1],  nan=0.0), dx=1.0/fs_hz)) if pk >= a else np.nan\n",
    "    auc_post = float(np.trapezoid(np.nan_to_num(p[pk:b+1], nan=0.0), dx=1.0/fs_hz)) if b >= pk else np.nan\n",
    "    total = (0 if not np.isfinite(auc_pre) else auc_pre) + (0 if not np.isfinite(auc_post) else auc_post)\n",
    "    base[\"auc_pre_j\"] = auc_pre\n",
    "    base[\"auc_post_j\"] = auc_post\n",
    "    base[\"work_early_pct\"] = float(100.0 * auc_pre / total) if total > 0 else np.nan\n",
    "\n",
    "    fall = p[pk:]\n",
    "    thr90 = 0.90 * p[pk]\n",
    "    thr10 = 0.10 * p[pk]\n",
    "    i90 = int(np.argmax(fall <= thr90)) if np.any(fall <= thr90) else 0\n",
    "    i10 = int(np.argmax(fall <= thr10)) if np.any(fall <= thr10) else len(fall)-1\n",
    "    base[\"decay_90_10_s\"] = (i10 - i90) / fs_hz if i10 > i90 else np.nan\n",
    "\n",
    "    finite = np.isfinite(p)\n",
    "    base[\"skewness\"] = float(stats.skew(p[finite])) if np.any(finite) else np.nan\n",
    "    base[\"kurtosis\"] = float(stats.kurtosis(p[finite], fisher=True)) if np.any(finite) else np.nan\n",
    "\n",
    "    x = p - np.nanmean(p)\n",
    "    X = np.abs(np.fft.rfft(np.nan_to_num(x)))\n",
    "    freqs = np.fft.rfftfreq(x.size, d=1.0/fs_hz)\n",
    "    base[\"spectral_centroid_hz\"] = float(np.sum(freqs * X) / max(1e-12, np.sum(X)))\n",
    "\n",
    "    return base\n",
    "\n",
    "metric_cols = [\n",
    "    (\"peak_power_w\",          \"REAL\"),\n",
    "    (\"time_to_peak_s\",        \"REAL\"),\n",
    "    (\"rpd_max_w_per_s\",       \"REAL\"),\n",
    "    (\"time_to_rpd_max_s\",     \"REAL\"),\n",
    "    (\"rise_time_10_90_s\",     \"REAL\"),\n",
    "    (\"fwhm_s\",                \"REAL\"),\n",
    "    (\"auc_j\",                 \"REAL\"),\n",
    "    (\"work_early_pct\",        \"REAL\"),\n",
    "    (\"decay_90_10_s\",         \"REAL\"),\n",
    "    (\"t_com_norm_0to1\",       \"REAL\"),\n",
    "    (\"skewness\",              \"REAL\"),\n",
    "    (\"kurtosis\",              \"REAL\"),\n",
    "    (\"spectral_centroid_hz\",  \"REAL\"),\n",
    "]\n",
    "\n",
    "tables = [\"CMJ\", \"DJ\", \"PPU\", \"SLV\"]   # add/remove as needed\n",
    "\n",
    "def column_exists(cur, table, col):\n",
    "    cur.execute(f\"PRAGMA table_info({table})\")\n",
    "    return any(row[1].lower() == col.lower() for row in cur.fetchall())\n",
    "\n",
    "def ensure_metric_columns(conn, table):\n",
    "    cur = conn.cursor()\n",
    "    for col, sqltype in metric_cols:\n",
    "        if not column_exists(cur, table, col):\n",
    "            cur.execute(f\"ALTER TABLE {table} ADD COLUMN {col} {sqltype}\")\n",
    "    conn.commit()\n",
    "\n",
    "def update_table_with_power(conn, table, fs_hz=1000.0):\n",
    "    \"\"\"\n",
    "    For each row in `table`, find '<trial_name>_Power.txt' and compute metrics.\n",
    "    For SLV we also match on 'side' in the WHERE clause.\n",
    "    \"\"\"\n",
    "    cur = conn.cursor()\n",
    "    ensure_metric_columns(conn, table)\n",
    "\n",
    "    # Pull identifying fields per table\n",
    "    if table == \"SLV\":\n",
    "        cur.execute(\"SELECT id, trial_name, side FROM SLV\")\n",
    "        rows = cur.fetchall()\n",
    "    else:\n",
    "        cur.execute(f\"SELECT id, trial_name FROM {table}\")\n",
    "        rows = [(r[0], r[1], None) for r in cur.fetchall()]  # pad with None for uniformity\n",
    "\n",
    "    # Prepare dynamic UPDATE\n",
    "    set_clause = \", \".join([f\"{c} = ?\" for (c, _) in metric_cols])\n",
    "    sql = f\"UPDATE {table} SET {set_clause} WHERE id = ?\"\n",
    "\n",
    "    updated = 0\n",
    "    missing = 0\n",
    "    failed  = 0\n",
    "\n",
    "    for rec in rows:\n",
    "        id_, trial_name, _side = rec\n",
    "        if not trial_name:\n",
    "            missing += 1\n",
    "            continue\n",
    "\n",
    "        power_path = os.path.join(folder_path, f\"{trial_name}_Power.txt\")\n",
    "        if not os.path.exists(power_path):\n",
    "            missing += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            s = load_power_txt(power_path)\n",
    "            m = analyze_power_curve_advanced(s, fs_hz=fs_hz)\n",
    "\n",
    "            # Order must match metric_cols\n",
    "            values = [\n",
    "                m.get(\"peak_power_w\"),\n",
    "                m.get(\"time_to_peak_s\"),\n",
    "                m.get(\"rpd_max_w_per_s\"),\n",
    "                m.get(\"time_to_rpd_max_s\"),\n",
    "                m.get(\"rise_time_10_90_s\"),\n",
    "                m.get(\"fwhm_s\"),\n",
    "                m.get(\"auc_j\"),\n",
    "                m.get(\"work_early_pct\"),\n",
    "                m.get(\"decay_90_10_s\"),\n",
    "                m.get(\"t_com_norm_0to1\"),\n",
    "                m.get(\"skewness\"),\n",
    "                m.get(\"kurtosis\"),\n",
    "                m.get(\"spectral_centroid_hz\"),\n",
    "                id_,\n",
    "            ]\n",
    "            cur.execute(sql, values)\n",
    "            updated += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  {table} id={id_} trial={trial_name}: power parse/analysis failed: {e}\")\n",
    "            failed += 1\n",
    "\n",
    "    conn.commit()\n",
    "    print(f\"{table}: updated={updated}, missing_power_files={missing}, failed={failed}\")\n",
    "\n",
    "# ---- run the updater ----\n",
    "for t in tables:\n",
    "    update_table_with_power(conn, t, fs_hz=1000.0)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n",
    "print(\"Data successfully inserted into the database.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing database at D:/Athletic Screen 2.0/Output Files/movement_database_v2.db\n",
      "Processing file: CMJ1.txt, Variables: [1.0, 20.0, 1880.5, 1816.18, 1035.43, 24.1]\n",
      "Processing file: CMJ2.txt, Variables: [1.0, 19.2, 2338.2, 1699.74, 1375.65, 29.97]\n",
      "Processing file: CMJ3.txt, Variables: [1.0, 20.0, 1880.5, 1816.18, 1035.43, 24.1]\n",
      "Processing file: DJ1.txt, Variables: [1.0, 17.8, 1170.4, 1505.16, 777.62, 0.54, 1.68, 15.0]\n",
      "Processing file: DJ2.txt, Variables: [1.0, 20.4, 1204.6, 1540.72, 781.86, 0.52, 1.98, 15.44]\n",
      "Processing file: DJ3.txt, Variables: [1.0, 20.2, 1367.6, 2284.35, 598.69, 0.31, 3.27, 17.53]\n",
      "Processing file: SLVL1.txt, Variables: [1.0, 9.8, 6168.0, 1837.5, 1486.5, 1236.1, 23.55]\n",
      "Processing file: SLVL2.txt, Variables: [1.0, 10.1, 6214.0, 1772.5, 1348.9, 1314.1, 22.72]\n",
      "Processing file: SLVR1.txt, Variables: [1.0, 12.2, 6554.0, 3239.2, 1751.5, 1849.4, 41.52]\n",
      "Processing file: SLVR2.txt, Variables: [1.0, 11.9, 6503.0, 1689.7, 1603.9, 1053.5, 21.66]\n",
      "Processing file: PPU1.txt, Variables: [1.0, 5.0, 711.2, 1097.14, 648.19, 9.12]\n",
      "Processing file: PPU2.txt, Variables: [1.0, 5.0, 444.3, 1013.55, 438.35, 5.69]\n",
      "CMJ: updated=3, missing_power_files=0, failed=0\n",
      "DJ: updated=3, missing_power_files=0, failed=0\n",
      "PPU: updated=2, missing_power_files=0, failed=0\n",
      "SLV: updated=4, missing_power_files=0, failed=0\n",
      "Data successfully inserted into the database.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T21:07:42.619655Z",
     "start_time": "2025-11-07T21:07:25.665699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creates full report for age group comparison\n",
    "\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from datetime import date\n",
    "import tempfile\n",
    "import docx2txt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os, re\n",
    "from scipy import integrate, stats\n",
    "import glob as globmod\n",
    "\n",
    "# -------- style to match your dark report --------\n",
    "plt.rcParams.update({\n",
    "    \"figure.facecolor\": \"#181818\",\n",
    "    \"axes.facecolor\"  : \"#303030\",\n",
    "    \"axes.edgecolor\"  : \"white\",\n",
    "    \"axes.labelcolor\" : \"slategrey\",\n",
    "    \"xtick.color\"     : \"lightgrey\",\n",
    "    \"ytick.color\"     : \"lightgrey\",\n",
    "    \"grid.color\"      : \"dimgrey\",\n",
    "    \"text.color\"      : \"white\",\n",
    "})\n",
    "# Corrected file paths with raw strings to handle backslashes properly\n",
    "client_db_path = r'D:\\Athletic Screen 2.0\\Output Files\\movement_database_v2.db'\n",
    "reference_db_path = r'D:\\Athletic Screen 2.0\\Output Files\\Athletic_Screen_College_data_v2.db'\n",
    "\n",
    "# Ensure the paths are valid and accessible\n",
    "if not os.path.exists(client_db_path):\n",
    "    print(f\"Client database not found at {client_db_path}\")\n",
    "if not os.path.exists(reference_db_path):\n",
    "    print(f\"Reference database not found at {reference_db_path}\")\n",
    "\n",
    "# Connect to the client and reference databases\n",
    "client_conn = sqlite3.connect(client_db_path)\n",
    "reference_conn = sqlite3.connect(reference_db_path)\n",
    "client_cursor = client_conn.cursor()\n",
    "reference_cursor = reference_conn.cursor()\n",
    "\n",
    "print(\"Databases opened successfully.\")\n",
    "\n",
    "# Fetch the client's name from the database (assuming the 'name' column is in all tables)\n",
    "client_cursor.execute(\"SELECT DISTINCT name FROM CMJ\")  # Change table if necessary\n",
    "client_name = client_cursor.fetchone()[0]  # Get the first row and first column\n",
    "print(f\"Client Name: {client_name}\")\n",
    "\n",
    "# ---------- build unique export paths (date-stamped, no overwrite) -----\n",
    "client_cursor.execute(\"SELECT MAX(date) FROM CMJ WHERE name = ?\", (client_name,))\n",
    "assessment_date = client_cursor.fetchone()[0]          # e.g. '2025-05-22'\n",
    "if not assessment_date:                                # fallback to today\n",
    "    assessment_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "reports_dir = r'G:\\My Drive\\Athletic Screen 2.0 Reports\\College Reports'\n",
    "os.makedirs(reports_dir, exist_ok=True)\n",
    "\n",
    "parts = client_name.split(', ')\n",
    "client_name_rev = f\"{parts[1]}_{parts[0]}\" if len(parts) == 2 else client_name\n",
    "base_name = f\"Athletic_Report_{client_name_rev}_{assessment_date}\"\n",
    "\n",
    "output_filename = os.path.join(reports_dir, base_name + \".docx\")\n",
    "img_output_directory = os.path.join(reports_dir, \"Images\", base_name)\n",
    "os.makedirs(img_output_directory, exist_ok=True)\n",
    "\n",
    "# auto-increment if the same file already exists\n",
    "counter = 1\n",
    "while os.path.exists(output_filename):\n",
    "    output_filename = os.path.join(\n",
    "        reports_dir, f\"{base_name}_{counter}.docx\"\n",
    "    )\n",
    "    img_output_directory = os.path.join(\n",
    "        reports_dir, \"Images\", f\"{base_name}_{counter}\"\n",
    "    )\n",
    "    os.makedirs(img_output_directory, exist_ok=True)\n",
    "    counter += 1\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Helper function to calculate percentile\n",
    "def calculate_percentile(value, reference_data):\n",
    "    return stats.percentileofscore(reference_data, value)\n",
    "\n",
    "# ─── UPDATED generate_bar_graph ────────────────────────────────────────────────\n",
    "def generate_bar_graph(variable, client_value, reference_data, title, tmpdirname):\n",
    "    \"\"\"\n",
    "    Blue bars  = reference distribution\n",
    "    ─ red      = client MAX (best trial **within the same movement table**)\n",
    "    ─ violet   = client MEAN (average of those trials)\n",
    "\n",
    "    • No code outside this function needs to change.\n",
    "    • If the caller still passes one score, that is fine; this function looks\n",
    "      up any matching trials on the same assessment day and combines them.\n",
    "    • RSI histograms use 0.25-wide bins so you see bars at 1.25, 1.50, 1.75 …\n",
    "    \"\"\"\n",
    "    import numpy as np, matplotlib.pyplot as plt, os, sqlite3, re\n",
    "\n",
    "    # ───────────── lookup: which movement table are we dealing with? ─────────\n",
    "    column_table_map = {\"CMJ\": [\"CMJ\"], \"DJ\": [\"DJ\"], \"PPU\": [\"PPU\"], \"SLV\": [\"SLV\"], \"NMT\": [\"NMT\"]}\n",
    "    table_guess = None\n",
    "    for tbl in column_table_map:          # CMJ, DJ, SLV, NMT\n",
    "        try:\n",
    "            reference_cursor.execute(f\"SELECT COUNT(*) FROM {tbl}\")\n",
    "            if reference_cursor.fetchone()[0] == len(reference_data):\n",
    "                table_guess = tbl\n",
    "                break\n",
    "        except sqlite3.OperationalError:\n",
    "            continue\n",
    "    if table_guess is None:               # fallback\n",
    "        for tbl in column_table_map:\n",
    "            try:\n",
    "                client_cursor.execute(f\"SELECT 1 FROM {tbl} LIMIT 1\")\n",
    "                table_guess = tbl\n",
    "                break\n",
    "            except sqlite3.OperationalError:\n",
    "                continue\n",
    "\n",
    "    # ───────────── gather all trials for this athlete / table / day ──────────\n",
    "    scores = []\n",
    "    if table_guess:\n",
    "        # Some tables (e.g., NMT) may not have trial_name. If so, fall back safely.\n",
    "        try:\n",
    "            client_cursor.execute(\n",
    "                f\"SELECT trial_name FROM {table_guess} WHERE name=? LIMIT 1\",\n",
    "                (client_name,),\n",
    "            )\n",
    "            row = client_cursor.fetchone()\n",
    "            date_prefix = None\n",
    "            if row and row[0]:\n",
    "                m = re.match(r\"(\\d{4}[-_]\\d{2}[-_]\\d{2})\", row[0])\n",
    "                date_prefix = m.group(1) if m else None\n",
    "    \n",
    "            if date_prefix:\n",
    "                q = f\"SELECT {variable} FROM {table_guess} WHERE name=? AND trial_name LIKE ?\"\n",
    "                client_cursor.execute(q, (client_name, f\"{date_prefix}%\"))\n",
    "            else:\n",
    "                q = f\"SELECT {variable} FROM {table_guess} WHERE name=?\"\n",
    "                client_cursor.execute(q, (client_name,))\n",
    "    \n",
    "            scores = [r[0] for r in client_cursor.fetchall() if r[0] is not None]\n",
    "        except sqlite3.OperationalError:\n",
    "            # No trial_name column (likely NMT) → just use the single value\n",
    "            scores = [client_value]\n",
    "    \n",
    "    # fallback if still empty\n",
    "    if not scores:\n",
    "        scores = [client_value]\n",
    "\n",
    "\n",
    "    scores = np.asarray(scores, dtype=float)\n",
    "    c_mean = scores.mean()\n",
    "    \n",
    "    # --- choose the extreme we draw as the red line ---------------------------\n",
    "    if variable.upper() == \"CT\":        # Contact-Time → use the LOWEST value\n",
    "        c_extreme = scores.min()\n",
    "        extreme_label = \"Client Min\"\n",
    "    else:                               # every other metric → highest value\n",
    "        c_extreme = scores.max()\n",
    "        extreme_label = \"Client Max\"\n",
    "\n",
    "    perc_mean = calculate_percentile(c_mean, reference_data)\n",
    "\n",
    "    # ──────────────────────────── plotting begins ────────────────────────────\n",
    "    plt.figure(facecolor=\"#181818\")\n",
    "    ax = plt.subplot(111, facecolor=\"#303030\")\n",
    "\n",
    "    reference_plotted = False  # flag to avoid double-plotting\n",
    "\n",
    "    # ---------- RSI special case: 0.25-wide bars & custom ticks -------------\n",
    "    if variable.upper() == \"RSI\":\n",
    "        lo = np.floor(reference_data.min() / 0.25) * 0.25\n",
    "        hi = np.ceil(reference_data.max() / 0.25) * 0.25\n",
    "        bins = np.arange(lo, hi + 0.25, 0.25)      # bin edges\n",
    "        centers = bins[:-1]                         # bar positions\n",
    "\n",
    "        counts, _ = np.histogram(reference_data, bins=bins)\n",
    "        ax.bar(\n",
    "            centers,\n",
    "            counts,\n",
    "            width=0.25,\n",
    "            align=\"edge\",\n",
    "            color=\"cornflowerblue\",\n",
    "            alpha=0.7,\n",
    "            edgecolor=\"white\",\n",
    "            label=\"Reference\",\n",
    "        )\n",
    "        # after counts, _ = np.histogram(...)\n",
    "        for x, h in zip(centers, counts):\n",
    "            if h == 0:                        # empty bin → draw thin outline\n",
    "                ax.bar(x, 1e-6, width=.25, align='edge',\n",
    "                       color='none', edgecolor='#404040', linewidth=.5)\n",
    "\n",
    "        ax.set_xticks(centers)\n",
    "        ax.set_xticklabels([f\"{x:.2f}\" for x in centers], color=\"lightgrey\")\n",
    "\n",
    "        reference_plotted = True  # we already drew the reference bars\n",
    "    else:\n",
    "        bins = 20  # default bin count\n",
    "\n",
    "    # ---------- draw reference histogram when not plotted above -------------\n",
    "    if not reference_plotted:\n",
    "        ax.hist(\n",
    "            reference_data,\n",
    "            bins=bins,\n",
    "            color=\"cornflowerblue\",\n",
    "            alpha=0.7,\n",
    "            edgecolor=\"white\",\n",
    "            label=\"Reference\",\n",
    "        )\n",
    "\n",
    "    # ---------- client mean / max lines -------------------------------------\n",
    "    ax.axvline(c_extreme, color=\"red\", ls=\"--\", lw=2, label=\"Client Max\")\n",
    "    ax.axvline(c_mean, color=\"violet\", ls=\"--\", lw=2, label=\"Client Mean\")\n",
    "\n",
    "    # ---------- cosmetics ----------------------------------------------------\n",
    "    ax.set_xlabel(variable.replace(\"_\", \" \"), color=\"slategrey\")\n",
    "    ax.set_ylabel(\"Frequency\", color=\"slategrey\")\n",
    "    ax.tick_params(axis=\"x\", colors=\"lightgrey\")\n",
    "    ax.tick_params(axis=\"y\", colors=\"lightgrey\")\n",
    "    ax.grid(color=\"dimgrey\")\n",
    "\n",
    "    txt = (\n",
    "        f\"Percentile (mean): {perc_mean:.1f}%\"\n",
    "        f\"\\nMean: {c_mean:.2f}\"\n",
    "        f\"\\nMax:  {c_extreme:.2f}\"\n",
    "    )\n",
    "    plt.text(\n",
    "        0.95,\n",
    "        0.05,\n",
    "        txt,\n",
    "        ha=\"right\",\n",
    "        va=\"bottom\",\n",
    "        transform=ax.transAxes,\n",
    "        color=\"white\",\n",
    "        fontsize=9,\n",
    "        backgroundcolor=\"#181818\",\n",
    "    )\n",
    "\n",
    "    ax.legend(facecolor=\"black\", edgecolor=\"grey\", prop={\"size\": \"small\"}, labelcolor=\"grey\")\n",
    "\n",
    "    # ---------- save ---------------------------------------------------------\n",
    "    out_path = os.path.join(tmpdirname, f\"{variable}_histogram.png\")\n",
    "    plt.savefig(out_path, bbox_inches=\"tight\", facecolor=\"#181818\")\n",
    "    plt.close()\n",
    "    return out_path\n",
    "\n",
    "# Function to generate scatter plot for CMJ\n",
    "def generate_scatter_plot(client_data, reference_data, x_var, y_var, title, tmpdirname):\n",
    "    plt.figure(facecolor='#181818', figsize=(6, 6))\n",
    "    ax = plt.subplot(111, facecolor='#303030')\n",
    "\n",
    "    # Create scatter plot for reference data (cornflower blue)\n",
    "    ax.scatter(reference_data[x_var], reference_data[y_var], label='Reference', alpha=0.5, color='cornflowerblue')\n",
    "\n",
    "    # Create scatter plot for client data (red)\n",
    "    ax.scatter(client_data[x_var], client_data[y_var], label='Client', color='red', edgecolors='black', s=100)\n",
    "\n",
    "    # Set axis labels, replacing underscores with spaces\n",
    "    ax.set_xlabel(x_var.replace('_', ' '), color='slategrey')\n",
    "    ax.set_ylabel(y_var.replace('_', ' '), color='slategrey')\n",
    "\n",
    "    # Dynamically set ticks and numbers to light grey\n",
    "    ax.tick_params(axis='x', colors='lightgrey')\n",
    "    ax.tick_params(axis='y', colors='lightgrey')\n",
    "\n",
    "    # Add vertical and horizontal reference lines (light grey)\n",
    "    ax.axvline(x=np.mean(reference_data[x_var]), color='lightgrey', linestyle='--', linewidth=1)\n",
    "    ax.axhline(y=np.mean(reference_data[y_var]), color='lightgrey', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Customize grid style\n",
    "    ax.grid(color='dimgrey')\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(facecolor='black', edgecolor='grey', prop={'size': 'small'}, labelcolor='grey')\n",
    "\n",
    "    # Save scatter plot to file\n",
    "    scatter_filename = os.path.join(tmpdirname, 'cmj_scatter.png')\n",
    "    plt.savefig(scatter_filename, bbox_inches='tight', facecolor='#181818')\n",
    "    plt.close()\n",
    "\n",
    "    return scatter_filename\n",
    "\n",
    "\n",
    "def load_power_txt(txt_path: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Parse exported power file (like your example). Returns a pandas Series of power.\n",
    "    Assumes: header lines, then a line starting with 'ITEM', then data rows:\n",
    "             <index>\\t<value>\n",
    "    Skips rows without a numeric second field.\n",
    "    \"\"\"\n",
    "    power_vals = []\n",
    "    in_data = False\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "            if not in_data:\n",
    "                if line.startswith(\"ITEM\"):\n",
    "                    in_data = True\n",
    "                continue\n",
    "            # from here on, try to parse the last field as a float\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = re.split(r\"\\t+\", line)\n",
    "            if len(parts) < 2:\n",
    "                # sometimes a blank power cell exists on the first data row — skip it\n",
    "                continue\n",
    "            try:\n",
    "                val = float(parts[-1])\n",
    "                power_vals.append(val)\n",
    "            except ValueError:\n",
    "                # non-numeric tail → ignore\n",
    "                continue\n",
    "    if not power_vals:\n",
    "        raise ValueError(f\"No power values parsed from {txt_path}\")\n",
    "    return pd.Series(power_vals, name=\"Power\")\n",
    "\n",
    "def analyze_power_curve(power: pd.Series, fs_hz: float = 1000.0) -> dict:\n",
    "    \"\"\"\n",
    "    Compute useful shape/temporal features.\n",
    "    power: Series of power (W). fs_hz: sampling rate (Hz). If unknown, 1000 Hz is typical.\n",
    "    Returns a dict of metrics.\n",
    "    \"\"\"\n",
    "    p = np.asarray(power, dtype=float)\n",
    "    n = p.size\n",
    "    t = np.arange(n) / fs_hz\n",
    "\n",
    "    # basic\n",
    "    p_peak_idx = int(np.nanargmax(p))\n",
    "    p_peak     = float(p[p_peak_idx])\n",
    "    t_peak     = float(t[p_peak_idx])\n",
    "\n",
    "    # onset/offset via % of peak (robust to baseline drift)\n",
    "    thr10 = 0.10 * p_peak\n",
    "    thr50 = 0.50 * p_peak\n",
    "    thr90 = 0.90 * p_peak\n",
    "\n",
    "    # first index above 10% of peak\n",
    "    try:\n",
    "        onset_idx = int(np.argmax(p >= thr10))\n",
    "    except ValueError:\n",
    "        onset_idx = 0\n",
    "    # first index after peak that falls below 10% (or end)\n",
    "    post = p[p_peak_idx:]\n",
    "    off_rel = np.argmax(post < thr10) if np.any(post < thr10) else (post.size - 1)\n",
    "    offset_idx = p_peak_idx + int(off_rel)\n",
    "\n",
    "    # 10–90% rise time on rising limb\n",
    "    rising = p[:p_peak_idx+1]\n",
    "    try:\n",
    "        i10 = int(np.argmax(rising >= thr10))\n",
    "        i90 = int(np.argmax(rising >= thr90))\n",
    "        rise_time = (i90 - i10) / fs_hz if i90 > i10 else np.nan\n",
    "        rise_slope = (0.8 * p_peak) / rise_time if rise_time and rise_time > 0 else np.nan\n",
    "    except ValueError:\n",
    "        i10 = i90 = None\n",
    "        rise_time = np.nan\n",
    "        rise_slope = np.nan\n",
    "\n",
    "    # FWHM (50% of peak) width\n",
    "    # left crossing\n",
    "    try:\n",
    "        left_idx  = int(np.argmax(rising >= thr50))\n",
    "    except ValueError:\n",
    "        left_idx = p_peak_idx\n",
    "    # right crossing\n",
    "    falling = p[p_peak_idx:]\n",
    "    try:\n",
    "        right_rel = int(np.argmax(falling <= thr50))\n",
    "        right_idx = p_peak_idx + right_rel\n",
    "    except ValueError:\n",
    "        right_idx = p_peak_idx\n",
    "    fwhm_sec = (right_idx - left_idx) / fs_hz if right_idx > left_idx else np.nan\n",
    "\n",
    "    # Work/impulse of power (area under curve) over the active window\n",
    "    a = max(0, onset_idx)\n",
    "    b = min(n - 1, max(offset_idx, p_peak_idx))\n",
    "    auc_joules = float(np.trapezoid(np.nan_to_num(p[a:b+1], nan=0.0), dx=1.0/fs_hz))\n",
    "\n",
    "    # timing “balance”: center of mass of power curve (0..1)\n",
    "    # (earlier vs. later power concentration)\n",
    "    weights = p[a:b+1].clip(min=0)\n",
    "    if weights.sum() > 0:\n",
    "        t_window = t[a:b+1]\n",
    "        t_com = float(np.sum(t_window * weights) / np.sum(weights))\n",
    "        t_com_norm = (t_com - t[a]) / max(1e-9, (t[b] - t[a]))\n",
    "    else:\n",
    "        t_com = np.nan\n",
    "        t_com_norm = np.nan\n",
    "\n",
    "    # variability around peak (local coefficient of variation in ±50 ms)\n",
    "    w = int(0.05 * fs_hz)\n",
    "    lo = max(0, p_peak_idx - w)\n",
    "    hi = min(n, p_peak_idx + w + 1)\n",
    "    local = p[lo:hi]\n",
    "    cv_local = float(np.std(local) / np.mean(local)) if np.mean(local) > 0 else np.nan\n",
    "\n",
    "    return {\n",
    "        \"n_samples\": n,\n",
    "        \"fs_hz\": fs_hz,\n",
    "        \"peak_power_w\": p_peak,\n",
    "        \"time_to_peak_s\": t_peak,\n",
    "        \"rise_time_10_90_s\": float(rise_time),\n",
    "        \"rise_slope_w_per_s\": float(rise_slope),\n",
    "        \"fwhm_s\": float(fwhm_sec),\n",
    "        \"auc_j\": auc_joules,\n",
    "        \"onset_idx\": a,\n",
    "        \"offset_idx\": b,\n",
    "        \"peak_idx\": p_peak_idx,\n",
    "        \"t_com_s\": t_com,\n",
    "        \"t_com_norm_0to1\": t_com_norm,\n",
    "        \"cv_local_peak\": cv_local,\n",
    "        \"i10_idx\": int(i10) if isinstance(i10, int) else None,\n",
    "        \"i90_idx\": int(i90) if isinstance(i90, int) else None,\n",
    "        \"left50_idx\": left_idx,\n",
    "        \"right50_idx\": right_idx,\n",
    "    }\n",
    "\n",
    "def plot_power_curve(power: pd.Series,\n",
    "                     metrics: dict,\n",
    "                     out_path: str,\n",
    "                     title: str = \"Power Curve\",\n",
    "                     annotate: bool = True):\n",
    "    \"\"\"\n",
    "    Plot power vs. time with annotations (peak, 10–90 rise, FWHM, AUC window).\n",
    "    \"\"\"\n",
    "    p = np.asarray(power, dtype=float)\n",
    "    t = np.arange(p.size) / metrics[\"fs_hz\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 3.6))\n",
    "    ax.plot(t, p, lw=2, label=\"Power\")\n",
    "    ax.grid(True)\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_ylabel(\"Power (W)\")\n",
    "    ax.set_title(title, pad=8, color=\"white\")\n",
    "\n",
    "    # annotate regions\n",
    "    a, b = metrics[\"onset_idx\"], metrics[\"offset_idx\"]\n",
    "    ax.axvspan(t[a], t[b], color=\"white\", alpha=0.07, label=\"active window\")\n",
    "\n",
    "    # peak\n",
    "    pk = metrics[\"peak_idx\"]\n",
    "    ax.plot([t[pk]], [p[pk]], \"o\", ms=6, color=\"tomato\", label=\"Peak\")\n",
    "    ax.axhline(p[pk]*0.5, ls=\"--\", lw=1, color=\"grey\")\n",
    "    ax.vlines([t[metrics[\"left50_idx\"]], t[metrics[\"right50_idx\"]]],\n",
    "              ymin=0, ymax=p[pk]*0.5, linestyles=\"--\", colors=\"grey\", lw=1)\n",
    "\n",
    "    # 10–90 rise (if available)\n",
    "    if metrics[\"i10_idx\"] is not None and metrics[\"i90_idx\"] is not None:\n",
    "        ax.plot([t[metrics[\"i10_idx\"]], t[metrics[\"i90_idx\"]]],\n",
    "                [p[metrics[\"i10_idx\"]], p[metrics[\"i90_idx\"]]],\n",
    "                lw=3, color=\"deepskyblue\", label=\"10–90% rise\")\n",
    "\n",
    "    if annotate:\n",
    "        txt = (f\"Peak: {metrics['peak_power_w']:.1f} W @ {metrics['time_to_peak_s']:.3f} s\"\n",
    "               f\"\\nRise 10–90: {metrics['rise_time_10_90_s']:.3f} s\"\n",
    "               f\"\\nFWHM: {metrics['fwhm_s']:.3f} s\"\n",
    "               f\"\\nWork (AUC): {metrics['auc_j']:.1f} J\"\n",
    "               f\"\\nTiming COM: {metrics['t_com_norm_0to1']:.2f} (0 early…1 late)\")\n",
    "        ax.text(0.99, 0.02, txt, ha=\"right\", va=\"bottom\",\n",
    "                transform=ax.transAxes, fontsize=9, color=\"white\",\n",
    "                bbox=dict(boxstyle=\"round,pad=0.25\", facecolor=\"#181818\", edgecolor=\"#444\"))\n",
    "\n",
    "    ax.legend(facecolor=\"black\", edgecolor=\"grey\", prop={\"size\": \"small\"}, labelcolor=\"grey\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def overlay_power_trials(traces: list[pd.Series],\n",
    "                         fs_hz: float,\n",
    "                         out_path: str,\n",
    "                         title: str = \"Power (all trials)\",\n",
    "                         align: str = \"peak\",          # \"peak\", \"onset10\", or \"none\"\n",
    "                         window_s: tuple[float, float] | None = (0.30, 0.40),\n",
    "                         show_mean: bool = True):\n",
    "    \"\"\"\n",
    "    Overlay multiple power traces and align them in time.\n",
    "\n",
    "    align:\n",
    "      - \"peak\"    → align each trial's max power to t=0\n",
    "      - \"onset10\" → align first sample ≥ 10% of that trial's peak to t=0\n",
    "      - \"none\"    → no alignment; left edges at t=0\n",
    "\n",
    "    window_s: (pre, post) seconds to show around t=0 (None to show full extent)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- convert input to arrays; find alignment index per trial --------------\n",
    "    arrs = [np.asarray(s, dtype=float) for s in traces if len(s) > 0]\n",
    "    if not arrs:\n",
    "        raise ValueError(\"overlay_power_trials: no non-empty traces provided\")\n",
    "\n",
    "    def _align_index(x: np.ndarray) -> int:\n",
    "        if align == \"none\":\n",
    "            return 0\n",
    "        # guard against NaNs\n",
    "        if not np.any(np.isfinite(x)):\n",
    "            return 0\n",
    "        # peak index\n",
    "        try:\n",
    "            pk = int(np.nanargmax(x))\n",
    "        except ValueError:\n",
    "            pk = 0\n",
    "        if align == \"peak\":\n",
    "            return pk\n",
    "        elif align == \"onset10\":\n",
    "            thr = 0.10 * (x[pk] if np.isfinite(x[pk]) else np.nanmax(x))\n",
    "            # first index >= 10% of that trial's peak\n",
    "            idx = int(np.argmax(x >= thr)) if np.any(x >= thr) else 0\n",
    "            return idx\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    align_idx = [ _align_index(x) for x in arrs ]\n",
    "\n",
    "    # --- build a padded matrix so all alignment points land at the same column\n",
    "    max_left  = max(align_idx)                                  # largest left padding needed\n",
    "    right_len = [len(x) - i for x, i in zip(arrs, align_idx)]   # samples from align idx to end\n",
    "    max_right = max(right_len)\n",
    "    L = max_left + max_right                                     # total aligned length\n",
    "\n",
    "    aligned = np.full((len(arrs), L), np.nan)\n",
    "    for r, (x, i0) in enumerate(zip(arrs, align_idx)):\n",
    "        start = max_left - i0\n",
    "        aligned[r, start:start+len(x)] = x\n",
    "\n",
    "    # --- time vector: t=0 at the common alignment column ---------------------\n",
    "    t = (np.arange(L) - max_left) / fs_hz\n",
    "\n",
    "    # --- optional cropping around t=0 ----------------------------------------\n",
    "    if window_s is not None:\n",
    "        pre, post = window_s\n",
    "        i_lo = max(0, int(np.floor((-pre)  * fs_hz)) + max_left)\n",
    "        i_hi = min(L, int(np.ceil( (post) * fs_hz)) + max_left)\n",
    "        aligned = aligned[:, i_lo:i_hi]\n",
    "        t = t[i_lo:i_hi]\n",
    "        \n",
    "    # --- plot ----------------------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(6, 3.6))\n",
    "    \n",
    "    # consistent y-limits (same scale across trials)\n",
    "    y_max = np.nanmax(aligned)\n",
    "    y_min = np.nanmin(aligned)\n",
    "    \n",
    "    # NEW: mask-based plotting so left/right NaNs don't truncate the curve visually\n",
    "    coverage_flags = []  # we'll use this to warn about very short traces\n",
    "    for i in range(aligned.shape[0]):\n",
    "        row = aligned[i]\n",
    "        mask = np.isfinite(row)\n",
    "        cov = np.count_nonzero(mask) / max(1, len(row))\n",
    "        coverage_flags.append(cov)\n",
    "        if np.count_nonzero(mask) >= 2:\n",
    "            ax.plot(t[mask], row[mask], lw=1.2, alpha=0.6)\n",
    "    \n",
    "    # optional: highlight which trials are very short (<50% of the window)\n",
    "    short = [idx for idx, c in enumerate(coverage_flags, start=1) if c < 0.5]\n",
    "    if short:\n",
    "        print(f\"⚠️ overlay_power_trials: {len(short)} trial(s) with <50% coverage in the window: {short}. \"\n",
    "              f\"Consider changing window_s or checking the export files.\")\n",
    "    \n",
    "    if show_mean:\n",
    "        mean_curve = np.nanmean(aligned, axis=0)\n",
    "        ax.plot(t, mean_curve, lw=2.2, color=\"cyan\", label=\"Mean\")\n",
    "    \n",
    "    # vertical line at alignment point\n",
    "    ax.axvline(0.0, color=\"grey\", lw=1, ls=\"--\",\n",
    "               label=(\"Aligned peak\" if align == \"peak\" else \"Aligned onset\"))\n",
    "    \n",
    "    ax.grid(True)\n",
    "    ax.set_xlabel(\"Time (s, aligned)\")\n",
    "    ax.set_ylabel(\"Power (W)\")\n",
    "    ax.set_title(title, pad=8, color=\"white\")\n",
    "    if show_mean:\n",
    "        ax.legend(facecolor=\"black\", edgecolor=\"grey\", prop={\"size\": \"small\"}, labelcolor=\"grey\")\n",
    "    \n",
    "    ax.set_ylim(y_min - 0.05 * abs(y_max - y_min), y_max + 0.05 * abs(y_max - y_min))\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def find_power_files(movement: str,\n",
    "                     base_dir: str = r'D:\\Athletic Screen 2.0\\Output Files') -> list[str]:\n",
    "    \"\"\"\n",
    "    Finds exported power files for a movement. Supports:\n",
    "      CMJ_Power.txt, CMJ1_Power.txt, CMJ2_Power.txt, ...\n",
    "      DJ_Power.txt,  DJ*_Power.txt\n",
    "      SLV_Power.txt, SLV*_Power.txt\n",
    "    Returns a sorted (unique) list of full paths.\n",
    "    \"\"\"\n",
    "    pats = [f\"{movement}_Power.txt\", f\"{movement}*_Power.txt\"]\n",
    "    files = []\n",
    "    for p in pats:\n",
    "        files += globmod.glob(os.path.join(base_dir, p))\n",
    "    return sorted(set(files))\n",
    "\n",
    "def build_aligned_matrix(traces: list[pd.Series],\n",
    "                         fs_hz: float,\n",
    "                         align: str = \"peak\",\n",
    "                         window_s: tuple[float, float] | None = (0.30, 0.40)):\n",
    "    \"\"\"\n",
    "    Returns (aligned 2D array [n_trials x T], time vector [T], indices_info[list]).\n",
    "    Time is centered so the chosen alignment point is at t=0.\n",
    "    \"\"\"\n",
    "    arrs = [np.asarray(s, dtype=float) for s in traces if len(s) > 0]\n",
    "    if not arrs:\n",
    "        raise ValueError(\"build_aligned_matrix: no non-empty traces\")\n",
    "\n",
    "    def _align_idx(x: np.ndarray) -> int:\n",
    "        if align == \"none\":\n",
    "            return 0\n",
    "        if not np.any(np.isfinite(x)):\n",
    "            return 0\n",
    "        pk = int(np.nanargmax(x))\n",
    "        if align == \"peak\":\n",
    "            return pk\n",
    "        elif align == \"onset10\":\n",
    "            thr = 0.10 * (x[pk] if np.isfinite(x[pk]) else np.nanmax(x))\n",
    "            return int(np.argmax(x >= thr)) if np.any(x >= thr) else 0\n",
    "        return 0\n",
    "\n",
    "    aidx = [_align_idx(x) for x in arrs]\n",
    "    max_left = max(aidx)\n",
    "    right_len = [len(x) - i for x, i in zip(arrs, aidx)]\n",
    "    max_right = max(right_len)\n",
    "    L = max_left + max_right\n",
    "\n",
    "    aligned = np.full((len(arrs), L), np.nan)\n",
    "    for r, (x, i0) in enumerate(zip(arrs, aidx)):\n",
    "        start = max_left - i0\n",
    "        aligned[r, start:start+len(x)] = x\n",
    "\n",
    "    t = (np.arange(L) - max_left) / fs_hz\n",
    "\n",
    "    if window_s is not None:\n",
    "        pre, post = window_s\n",
    "        i_lo = max(0, int(np.floor((-pre)  * fs_hz)) + max_left)\n",
    "        i_hi = min(L, int(np.ceil( (post) * fs_hz)) + max_left)\n",
    "        return aligned[:, i_lo:i_hi], t[i_lo:i_hi], aidx\n",
    "    return aligned, t, aidx\n",
    "\n",
    "def mean_aligned_curve(traces: list[pd.Series],\n",
    "                       fs_hz: float,\n",
    "                       align: str = \"peak\",\n",
    "                       window_s: tuple[float, float] | None = (0.30, 0.40)) -> pd.Series:\n",
    "    aligned, t, _ = build_aligned_matrix(traces, fs_hz, align, window_s)\n",
    "    mean_curve = np.nanmean(aligned, axis=0)\n",
    "    return pd.Series(mean_curve, index=t, name=\"Power\")\n",
    "\n",
    "def analyze_power_curve_advanced(power: pd.Series, fs_hz: float = 1000.0) -> dict:\n",
    "    \"\"\"\n",
    "    Extends your analyze_power_curve with additional, practical features:\n",
    "      • rpd_max (max rate of power development) & time to RPDmax\n",
    "      • AUC early (pre-peak) / late (post-peak), % early work\n",
    "      • decay_90_10 on falling limb\n",
    "      • skewness, kurtosis\n",
    "      • spectral centroid (how ‘fast’ the curve is in frequency domain)\n",
    "    \"\"\"\n",
    "    base = analyze_power_curve(power, fs_hz)\n",
    "    p = np.asarray(power, dtype=float)\n",
    "    n = p.size\n",
    "    t = np.arange(n) / fs_hz\n",
    "\n",
    "    # RPD\n",
    "    dp = np.gradient(p, 1.0/fs_hz)\n",
    "    rpd_max = float(np.nanmax(dp))\n",
    "    rpd_idx = int(np.nanargmax(dp))\n",
    "    base[\"rpd_max_w_per_s\"]   = rpd_max\n",
    "    base[\"time_to_rpd_max_s\"] = rpd_idx / fs_hz\n",
    "\n",
    "    # Early/late work around peak (use on/peak/off from base)\n",
    "    a, b, pk = base[\"onset_idx\"], base[\"offset_idx\"], base[\"peak_idx\"]\n",
    "    auc_pre  = float(np.trapezoid(np.nan_to_num(p[a:pk+1],  nan=0.0), dx=1.0/fs_hz)) if pk >= a else np.nan\n",
    "    auc_post = float(np.trapezoid(np.nan_to_num(p[pk:b+1], nan=0.0), dx=1.0/fs_hz)) if b >= pk else np.nan\n",
    "    total    = (auc_pre if np.isfinite(auc_pre) else 0) + (auc_post if np.isfinite(auc_post) else 0)\n",
    "    base[\"auc_pre_j\"]      = auc_pre\n",
    "    base[\"auc_post_j\"]     = auc_post\n",
    "    base[\"work_early_pct\"] = float(100.0 * auc_pre / total) if total > 0 else np.nan\n",
    "\n",
    "    # Decay time 90→10% of peak on falling limb\n",
    "    peak_val = p[pk]\n",
    "    fall = p[pk:]\n",
    "    thr90 = 0.90 * peak_val\n",
    "    thr10 = 0.10 * peak_val\n",
    "    i90 = int(np.argmax(fall <= thr90)) if np.any(fall <= thr90) else 0\n",
    "    i10 = int(np.argmax(fall <= thr10)) if np.any(fall <= thr10) else len(fall)-1\n",
    "    base[\"decay_90_10_s\"] = (i10 - i90) / fs_hz if i10 > i90 else np.nan\n",
    "\n",
    "    # Shape stats\n",
    "    finite = np.isfinite(p)\n",
    "    base[\"skewness\"] = float(stats.skew(p[finite])) if np.any(finite) else np.nan\n",
    "    base[\"kurtosis\"] = float(stats.kurtosis(p[finite], fisher=True)) if np.any(finite) else np.nan\n",
    "\n",
    "    # Spectral centroid\n",
    "    x = p - np.nanmean(p)\n",
    "    X = np.abs(np.fft.rfft(np.nan_to_num(x)))\n",
    "    freqs = np.fft.rfftfreq(x.size, d=1.0/fs_hz)\n",
    "    base[\"spectral_centroid_hz\"] = float(np.sum(freqs * X) / max(1e-12, np.sum(X)))\n",
    "\n",
    "    return base\n",
    "\n",
    "def add_power_analysis_section(doc: Document,\n",
    "                               movement: str,\n",
    "                               traces: list[pd.Series],\n",
    "                               fs_hz: float,\n",
    "                               tmpdirname: str,\n",
    "                               reference_cursor,\n",
    "                               reference_table: str):\n",
    "    \"\"\"\n",
    "    1) Overlay (peaks aligned) + mean curve\n",
    "    2) Annotated mean power curve\n",
    "    3) Table of per-trial metrics + Mean/SD\n",
    "    4) Adds reference percentile for peak power (vs PP_FORCEPLATE in ref DB)\n",
    "    \"\"\"\n",
    "    # --- 1) Overlay (aligned at peak) ---------------------------------------\n",
    "    overlay_png = os.path.join(tmpdirname, f\"{movement}_power_overlay.png\")\n",
    "    overlay_power_trials(\n",
    "        traces, fs_hz=fs_hz, out_path=overlay_png,\n",
    "        title=f\"{movement} Power – All Trials (peaks aligned)\",\n",
    "        align=\"peak\", window_s=(0.30, 0.40), show_mean=True\n",
    "    )\n",
    "    doc.add_paragraph(\"Power Curves (aligned at peak)\", style=\"Heading 2\")\n",
    "    doc.add_picture(overlay_png, width=Inches(6))\n",
    "\n",
    "    # --- 2) Mean curve (aligned) + annotated plot ---------------------------\n",
    "    mean_series = mean_aligned_curve(traces, fs_hz, align=\"peak\", window_s=(0.30, 0.40))\n",
    "    mean_metrics = analyze_power_curve_advanced(mean_series, fs_hz=fs_hz)\n",
    "\n",
    "    mean_png = os.path.join(tmpdirname, f\"{movement}_power_mean_annotated.png\")\n",
    "    plot_power_curve(mean_series, mean_metrics, mean_png,\n",
    "                     title=f\"{movement} – Mean Power (aligned)\")\n",
    "    doc.add_paragraph(\"Mean Power Curve (annotated)\", style=\"Heading 3\")\n",
    "    doc.add_picture(mean_png, width=Inches(6))\n",
    "\n",
    "    # --- 3) Per-trial metrics & summary -------------------------------------\n",
    "    per = [analyze_power_curve_advanced(s, fs_hz=fs_hz) for s in traces]\n",
    "    df  = pd.DataFrame(per)\n",
    "\n",
    "    # Pick the important rows for the document (order here = row order in table)\n",
    "    metric_rows = [\n",
    "        (\"Peak Power (W)\",            \"peak_power_w\",        \"{:.0f}\"),\n",
    "        (\"Time to Peak (s)\",          \"time_to_peak_s\",      \"{:.3f}\"),\n",
    "        (\"RPD max (W/s)\",             \"rpd_max_w_per_s\",     \"{:.0f}\"),\n",
    "        (\"Time to RPD max (s)\",       \"time_to_rpd_max_s\",   \"{:.3f}\"),\n",
    "        (\"Rise 10–90% (s)\",           \"rise_time_10_90_s\",   \"{:.3f}\"),\n",
    "        (\"FWHM (s)\",                  \"fwhm_s\",              \"{:.3f}\"),\n",
    "        (\"Work (AUC, J)\",             \"auc_j\",               \"{:.0f}\"),\n",
    "        (\"Early work (%)\",            \"work_early_pct\",      \"{:.1f}\"),\n",
    "        (\"Decay 90→10% (s)\",          \"decay_90_10_s\",       \"{:.3f}\"),\n",
    "        (\"Timing CoM (0…1)\",          \"t_com_norm_0to1\",     \"{:.2f}\"),\n",
    "        (\"Skewness\",                  \"skewness\",            \"{:.2f}\"),\n",
    "        (\"Kurtosis\",                  \"kurtosis\",            \"{:.2f}\"),\n",
    "        (\"Spectral centroid (Hz)\",    \"spectral_centroid_hz\",\"{:.2f}\"),\n",
    "    ]\n",
    "\n",
    "    # table: Metric | Trial1 | Trial2 | ... | Mean | SD\n",
    "    tbl = doc.add_table(rows=1 + len(metric_rows), cols=2 + len(traces))\n",
    "    tbl.style = \"Light List\" if \"Light List\" in [s.name for s in doc.styles] else tbl.style\n",
    "    # header\n",
    "    hdr = tbl.rows[0].cells\n",
    "    hdr[0].text = \"Metric\"\n",
    "    for i in range(len(traces)):\n",
    "        hdr[1+i].text = f\"Trial {i+1}\"\n",
    "    hdr[-1].text = \"Mean ± SD\"\n",
    "\n",
    "    # body\n",
    "    for r, (label, key, fmt) in enumerate(metric_rows, start=1):\n",
    "        row_cells = tbl.rows[r].cells\n",
    "        row_cells[0].text = label\n",
    "        vals = df.get(key, pd.Series([np.nan]*len(traces))).values\n",
    "        for i, v in enumerate(vals):\n",
    "            row_cells[1+i].text = (fmt.format(v) if np.isfinite(v) else \"—\")\n",
    "        mu = np.nanmean(vals)\n",
    "        sd = np.nanstd(vals, ddof=1) if np.count_nonzero(np.isfinite(vals)) > 1 else np.nan\n",
    "        row_cells[-1].text = (f\"{fmt.format(mu)} ± {fmt.format(sd)}\"\n",
    "                              if np.isfinite(mu) and np.isfinite(sd) else\n",
    "                              (fmt.format(mu) if np.isfinite(mu) else \"—\"))\n",
    "\n",
    "    # small spacer\n",
    "    doc.add_paragraph(\"\")\n",
    "\n",
    "    # --- 4) Reference percentile for peak power (vs. DB PP_FORCEPLATE) ------\n",
    "    try:\n",
    "        reference_cursor.execute(f\"SELECT PP_FORCEPLATE FROM {reference_table} WHERE PP_FORCEPLATE IS NOT NULL\")\n",
    "        ref_pp = np.array([r[0] for r in reference_cursor.fetchall()], dtype=float)\n",
    "    except sqlite3.OperationalError:\n",
    "        ref_pp = np.array([])\n",
    "\n",
    "    if ref_pp.size:\n",
    "        # use the highest peak among trials (or mean peak if you prefer)\n",
    "        trial_peaks = df[\"peak_power_w\"].values\n",
    "        best_peak   = float(np.nanmax(trial_peaks)) if trial_peaks.size else np.nan\n",
    "        pctl        = percentile_vs_reference(best_peak, ref_pp) if np.isfinite(best_peak) else np.nan\n",
    "        doc.add_paragraph(f\"Reference percentile (peak power): {pctl:.1f}%\", style=\"Intense Quote\")\n",
    "\n",
    "def percentile_vs_reference(value: float, reference_values: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Wrapper for percentile (1..99) using scipy-like methodology.\n",
    "    \"\"\"\n",
    "    return stats.percentileofscore(reference_values, value)\n",
    "\n",
    "# Modified function to generate a histogram comparing left and right leg data\n",
    "def generate_slv_histogram(variable, left_value, right_value,\n",
    "                           reference_data, title, tmpdirname):\n",
    "    \"\"\"\n",
    "    Blue bars  = reference distribution\n",
    "    ─ green    = client LEFT (latest trial value you passed in)\n",
    "    ─ orange   = client RIGHT\n",
    "\n",
    "    Text box shows, for each side:\n",
    "        • mean across all trials from the same assessment day\n",
    "        • max across those trials\n",
    "        • percentile of that mean vs. the reference distribution\n",
    "    \"\"\"\n",
    "    import numpy as np, matplotlib.pyplot as plt, os, re, sqlite3\n",
    "\n",
    "    # ── helper – get all trials for a given side on the same assessment day ──\n",
    "    def _fetch_side_vals(side):\n",
    "        # detect yyyy-mm-dd prefix in the first trial_name for this side\n",
    "        client_cursor.execute(\n",
    "            \"SELECT trial_name FROM SLV WHERE name=? AND side=? LIMIT 1\",\n",
    "            (client_name, side)\n",
    "        )\n",
    "        row = client_cursor.fetchone()\n",
    "        date_prefix = None\n",
    "        if row and row[0]:\n",
    "            m = re.match(r'(\\d{4}[-_]\\d{2}[-_]\\d{2})', row[0])\n",
    "            date_prefix = m.group(1) if m else None\n",
    "\n",
    "        if date_prefix:\n",
    "            q = f\"SELECT {variable} FROM SLV WHERE name=? AND side=? AND trial_name LIKE ?\"\n",
    "            client_cursor.execute(q, (client_name, side, f'{date_prefix}%'))\n",
    "        else:\n",
    "            q = f\"SELECT {variable} FROM SLV WHERE name=? AND side=?\"\n",
    "            client_cursor.execute(q, (client_name, side))\n",
    "\n",
    "        return [r[0] for r in client_cursor.fetchall() if r[0] is not None]\n",
    "\n",
    "    # pull all trials for each side; fall back to the single value passed in\n",
    "    left_vals  = np.asarray(_fetch_side_vals('Left')  or [left_value],  dtype=float)\n",
    "    right_vals = np.asarray(_fetch_side_vals('Right') or [right_value], dtype=float)\n",
    "\n",
    "    left_mean,  left_max  = left_vals.mean(),  left_vals.max()\n",
    "    right_mean, right_max = right_vals.mean(), right_vals.max()\n",
    "\n",
    "    left_pct  = calculate_percentile(left_mean,  reference_data)\n",
    "    right_pct = calculate_percentile(right_mean, reference_data)\n",
    "\n",
    "    # ── plot ────────────────────────────────────────────────────────────────\n",
    "    plt.figure(facecolor='#181818')\n",
    "    ax = plt.subplot(111, facecolor='#303030')\n",
    "\n",
    "    ax.hist(reference_data, bins=20, color='cornflowerblue',\n",
    "            alpha=0.7, edgecolor='white', label='Reference')\n",
    "\n",
    "    ax.axvline(left_value,  color='green',  ls='--', lw=2, label='Left (latest)')\n",
    "    ax.axvline(right_value, color='orange', ls='--', lw=2, label='Right (latest)')\n",
    "\n",
    "    ax.set_xlabel(variable.replace('_', ' '), color='slategrey')\n",
    "    ax.set_ylabel('Frequency',               color='slategrey')\n",
    "    ax.tick_params(axis='x', colors='lightgrey')\n",
    "    ax.tick_params(axis='y', colors='lightgrey')\n",
    "    ax.grid(color='dimgrey')\n",
    "\n",
    "    txt = (\n",
    "        f'LEFT  – mean: {left_mean:.2f}\\n'\n",
    "        f'        max:  {left_max:.2f}\\n'\n",
    "        f'        %ile: {left_pct:.1f}\\n'\n",
    "        f'RIGHT – mean: {right_mean:.2f}\\n'\n",
    "        f'        max:  {right_max:.2f}\\n'\n",
    "        f'        %ile: {right_pct:.1f}'\n",
    "    )\n",
    "    plt.text(0.95, 0.05, txt, ha='right', va='bottom',\n",
    "             transform=ax.transAxes, color='white', fontsize=9,\n",
    "             backgroundcolor='#181818')\n",
    "\n",
    "    ax.legend(facecolor='black', edgecolor='grey',\n",
    "              prop={'size': 'small'}, labelcolor='grey')\n",
    "\n",
    "    out_path = os.path.join(tmpdirname, f'{variable}_histogram_slv.png')\n",
    "    plt.savefig(out_path, bbox_inches='tight', facecolor='#181818')\n",
    "    plt.close()\n",
    "    return out_path\n",
    "\n",
    "# Prepare the document\n",
    "doc = Document()\n",
    "doc.add_picture(\"8ctane Baseball - Black abd Blue BG.jpeg\", width=Inches(4.0))  # Replace with your logo path\n",
    "doc.paragraphs[-1].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "\n",
    "# Adding player name and date\n",
    "doc.add_paragraph(f\"Player's Name: {client_name}\")  # Replace client_name with dynamic value\n",
    "doc.add_paragraph(f\"Date: {date.today().strftime('%B %d, %Y')}\")\n",
    "\n",
    "# Create a temporary directory to store images\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    # List of movements to process\n",
    "    movements = ['CMJ', 'DJ', 'PPU', 'SLV', 'NMT']\n",
    "    \n",
    "    for movement in movements:\n",
    "        # Add movement title\n",
    "        doc.add_paragraph(f\"{movement} Report\", style='Title')\n",
    "        doc.add_paragraph(f\"This section includes percentile reports and comparisons for {movement}.\", style='Heading 2')\n",
    "\n",
    "        if movement == 'CMJ':\n",
    "            # Fetch CMJ data for the client\n",
    "            client_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg,\n",
    "                       Force_at_PP, Vel_at_PP\n",
    "                FROM CMJ WHERE name = ?\n",
    "            \"\"\", (client_name,))\n",
    "            client_cmj_data = client_cursor.fetchone()\n",
    "            reference_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg,\n",
    "                       Force_at_PP, Vel_at_PP\n",
    "                FROM CMJ\n",
    "            \"\"\")\n",
    "            reference_cmj_data = np.array(reference_cursor.fetchall())\n",
    "            \n",
    "            # --- CMJ power files & analysis ---\n",
    "            cmj_files = find_power_files(\"CMJ\")\n",
    "            if cmj_files:\n",
    "                cmj_traces = [load_power_txt(pf) for pf in cmj_files]\n",
    "                add_power_analysis_section(\n",
    "                    doc, movement=\"CMJ\", traces=cmj_traces, fs_hz=1000,\n",
    "                    tmpdirname=tmpdirname, reference_cursor=reference_cursor,\n",
    "                    reference_table=\"CMJ\"\n",
    "                )\n",
    "\n",
    "            # Ensure data exists before proceeding\n",
    "            if client_cmj_data and reference_cmj_data.size > 0:\n",
    "                # Generate bar graphs for each variable in CMJ\n",
    "                variables = ['JH_IN', 'PP_FORCEPLATE', 'PP_W_per_kg', 'Force_at_PP', 'Vel_at_PP']\n",
    "                for i, var in enumerate(variables):\n",
    "                    # Format the variable name by removing underscores\n",
    "                    formatted_var = var.replace('_', ' ')\n",
    "                    \n",
    "                    # Add variable title before the graph\n",
    "                    doc.add_paragraph(f\"{formatted_var} Comparison\", style='Heading 2')\n",
    "                    \n",
    "                    # Generate the bar graph and add to document\n",
    "                    bar_image = generate_bar_graph(var, client_cmj_data[i], reference_cmj_data[:, i], f'{formatted_var} Comparison', tmpdirname)\n",
    "                    doc.add_picture(bar_image, width=Inches(6))\n",
    "                \n",
    "                # Generate scatter plot for CMJ (Force_Peak_Power vs. Velo_Peak_Power)\n",
    "                client_cmj_dict = {'Force_at_PP': client_cmj_data[3],\n",
    "                   'Vel_at_PP':   client_cmj_data[4]}\n",
    "                reference_cmj_dict = pd.DataFrame(reference_cmj_data, columns=variables)\n",
    "                \n",
    "                # Add scatter plot title and image\n",
    "                doc.add_paragraph(\"Force vs. Velocity Scatter Plot\", style='Heading 2')\n",
    "                scatter_image = generate_scatter_plot(client_cmj_dict, reference_cmj_dict,\n",
    "                                                      'Force_at_PP', 'Vel_at_PP',\n",
    "                                                      'CMJ: Force vs. Velocity', tmpdirname)\n",
    "                doc.add_picture(scatter_image, width=Inches(6))\n",
    "    \n",
    "        # ───────────────────────── DJ ─────────────────────────        \n",
    "        elif movement == 'DJ':\n",
    "            client_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg, Force_at_PP, Vel_at_PP, CT, RSI\n",
    "                FROM DJ\n",
    "                WHERE name = ?\n",
    "            \"\"\", (client_name,))\n",
    "            client_dj_data = client_cursor.fetchone()\n",
    "\n",
    "            # ── REFERENCE (robust: pandas + print counts) ─────────────────────\n",
    "            dj_vars = ['JH_IN', 'PP_FORCEPLATE', 'PP_W_per_kg',\n",
    "                       'Force_at_PP', 'Vel_at_PP', 'CT', 'RSI']\n",
    "            ref_sql = f\"SELECT {', '.join(dj_vars)} FROM DJ\"\n",
    "            reference_dj_df = pd.read_sql_query(ref_sql, reference_conn)\n",
    "            print(f\"DJ reference rows: {len(reference_dj_df)}\")\n",
    "            reference_dj_data = reference_dj_df.to_numpy(dtype=float, copy=False)\n",
    "\n",
    "            # ── POWER OVERLAY (files like *DJ*_Power*.txt or DJ_Power*.txt) ───\n",
    "            power_dir = r\"D:\\Athletic Screen 2.0\\Output Files\"\n",
    "            dj_power_files = (sorted(globmod.glob(os.path.join(power_dir, \"*DJ*_Power*.txt\"))) or\n",
    "                  sorted(globmod.glob(os.path.join(power_dir, \"DJ_Power*.txt\"))))\n",
    "            # --- DJ power files & analysis ---\n",
    "            dj_files = find_power_files(\"DJ\")\n",
    "            if dj_files:\n",
    "                dj_traces = [load_power_txt(pf) for pf in dj_files]\n",
    "                add_power_analysis_section(\n",
    "                    doc, movement=\"DJ\", traces=dj_traces, fs_hz=1000,\n",
    "                    tmpdirname=tmpdirname, reference_cursor=reference_cursor,\n",
    "                    reference_table=\"DJ\"\n",
    "                )\n",
    "\n",
    "            # ── BAR GRAPHS + SCATTER (with reference) ─────────────────────────\n",
    "            if client_dj_data:\n",
    "                for i, var in enumerate(dj_vars):\n",
    "                    doc.add_paragraph(f\"{var.replace('_',' ')} Comparison\", style='Heading 2')\n",
    "                    ref_col = reference_dj_data[:, i] if len(reference_dj_df) else np.array([])\n",
    "                    bar = generate_bar_graph(var, client_dj_data[i], ref_col,\n",
    "                                             f'{var} Comparison', tmpdirname)\n",
    "                    doc.add_picture(bar, width=Inches(6))\n",
    "\n",
    "                client_dj_dict = {'Force_at_PP': client_dj_data[3],\n",
    "                                  'Vel_at_PP'  : client_dj_data[4]}\n",
    "                doc.add_paragraph(\"Force vs. Velocity Scatter Plot\", style='Heading 2')\n",
    "                dj_scatter = generate_scatter_plot(\n",
    "                    client_dj_dict,\n",
    "                    reference_dj_df if not reference_dj_df.empty else pd.DataFrame(columns=dj_vars),\n",
    "                    'Force_at_PP', 'Vel_at_PP',\n",
    "                    'DJ: Force vs. Velocity', tmpdirname\n",
    "                )\n",
    "                doc.add_picture(dj_scatter, width=Inches(6))\n",
    "            else:\n",
    "                print(\"No DJ client row found.\")\n",
    "                \n",
    "        # ───────────────────────── PPU ───────────────────────   \n",
    "        elif movement == 'PPU':\n",
    "            # Fetch PPU data for the client\n",
    "            client_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg,\n",
    "                       Force_at_PP, Vel_at_PP\n",
    "                FROM PPU WHERE name = ?\n",
    "            \"\"\", (client_name,))\n",
    "            client_ppu_data = client_cursor.fetchone()\n",
    "\n",
    "            # Reference pull\n",
    "            reference_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg,\n",
    "                       Force_at_PP, Vel_at_PP\n",
    "                FROM PPU\n",
    "            \"\"\")\n",
    "            reference_ppu_data = np.array(reference_cursor.fetchall(), dtype=float)\n",
    "\n",
    "            # --- PPU power files & analysis (optional) ---\n",
    "            ppu_files = find_power_files(\"PPU\")\n",
    "            if ppu_files:\n",
    "                ppu_traces = [load_power_txt(pf) for pf in ppu_files]\n",
    "                add_power_analysis_section(\n",
    "                    doc, movement=\"PPU\", traces=ppu_traces, fs_hz=1000,\n",
    "                    tmpdirname=tmpdirname, reference_cursor=reference_cursor,\n",
    "                    reference_table=\"PPU\"\n",
    "                )\n",
    "\n",
    "            # Ensure data exists before proceeding\n",
    "            if client_ppu_data is not None and reference_ppu_data.size > 0:\n",
    "                variables = ['JH_IN', 'PP_FORCEPLATE', 'PP_W_per_kg', 'Force_at_PP', 'Vel_at_PP']\n",
    "\n",
    "                # Percentile histograms (same as CMJ)\n",
    "                for i, var in enumerate(variables):\n",
    "                    formatted_var = var.replace('_', ' ')\n",
    "                    doc.add_paragraph(f\"{formatted_var} Comparison\", style='Heading 2')\n",
    "                    bar_image = generate_bar_graph(\n",
    "                        var,\n",
    "                        float(client_ppu_data[i]),\n",
    "                        reference_ppu_data[:, i],\n",
    "                        f'{formatted_var} Comparison',\n",
    "                        tmpdirname\n",
    "                    )\n",
    "                    doc.add_picture(bar_image, width=Inches(6))\n",
    "\n",
    "                # Force–Velocity scatter (same as CMJ)\n",
    "                client_ppu_dict = {\n",
    "                    'Force_at_PP': float(client_ppu_data[3]),\n",
    "                    'Vel_at_PP'  : float(client_ppu_data[4]),\n",
    "                }\n",
    "                reference_ppu_df = pd.DataFrame(reference_ppu_data, columns=variables)\n",
    "                doc.add_paragraph(\"Force vs. Velocity Scatter Plot\", style='Heading 2')\n",
    "                ppu_scatter = generate_scatter_plot(\n",
    "                    client_ppu_dict, reference_ppu_df,\n",
    "                    'Force_at_PP', 'Vel_at_PP',\n",
    "                    'PPU: Force vs. Velocity', tmpdirname\n",
    "                )\n",
    "                doc.add_picture(ppu_scatter, width=Inches(6))\n",
    "            else:\n",
    "                print(\"PPU: missing client or reference data; skipping figure set.\")\n",
    "\n",
    "        # ───────────────────────── SLV ───────────────────────        \n",
    "        elif movement == 'SLV':\n",
    "            # ── CLIENT ROWS (Left/Right) ───────────────────────────────────────\n",
    "            client_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg, Force_at_PP, Vel_at_PP\n",
    "                FROM SLV WHERE name = ? AND side = 'Left'\n",
    "            \"\"\", (client_name,))\n",
    "            client_slvl_data = client_cursor.fetchone()\n",
    "\n",
    "            client_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg, Force_at_PP, Vel_at_PP\n",
    "                FROM SLV WHERE name = ? AND side = 'Right'\n",
    "            \"\"\", (client_name,))\n",
    "            client_slvr_data = client_cursor.fetchone()\n",
    "\n",
    "            # ── REFERENCE (keep side to filter/inspect if needed) ─────────────\n",
    "            slv_vars_no_side = ['JH_IN', 'PP_FORCEPLATE', 'PP_W_per_kg', 'Force_at_PP', 'Vel_at_PP']\n",
    "            ref_sql = f\"SELECT side, {', '.join(slv_vars_no_side)} FROM SLV\"\n",
    "            reference_slv_df = pd.read_sql_query(ref_sql, reference_conn)\n",
    "            print(f\"SLV reference rows: {len(reference_slv_df)}\")\n",
    "\n",
    "            # ── POWER OVERLAYS (Left & Right) ─────────────────────────────────\n",
    "            # We try common patterns. Adjust to your actual export names if needed.\n",
    "            power_dir = r\"D:\\Athletic Screen 2.0\\Output Files\"\n",
    "\n",
    "            # Left trials\n",
    "            slv_left_files = (sorted(globmod.glob(os.path.join(power_dir, \"*SLVL*_Power*.txt\"))) or\n",
    "                  sorted(globmod.glob(os.path.join(power_dir, \"SLV_Power_Left*.txt\"))) or\n",
    "                  sorted(globmod.glob(os.path.join(power_dir, \"SLV_Left*_Power*.txt\"))))\n",
    "\n",
    "            # Right trials\n",
    "            slv_right_files = (sorted(globmod.glob(os.path.join(power_dir, \"*SLVR*_Power*.txt\"))) or\n",
    "                   sorted(globmod.glob(os.path.join(power_dir, \"SLV_Power_Right*.txt\"))) or\n",
    "                   sorted(globmod.glob(os.path.join(power_dir, \"SLV_Right*_Power*.txt\"))))\n",
    "\n",
    "            # --- SLV power files & analysis ---\n",
    "            slv_files = find_power_files(\"SLV\")\n",
    "            if slv_files:\n",
    "                slv_traces = [load_power_txt(pf) for pf in slv_files]\n",
    "                add_power_analysis_section(\n",
    "                    doc, movement=\"SLV\", traces=slv_traces, fs_hz=1000,\n",
    "                    tmpdirname=tmpdirname, reference_cursor=reference_cursor,\n",
    "                    reference_table=\"SLV\"\n",
    "                )\n",
    "\n",
    "            # ── HISTOGRAMS + SCATTER (with reference) ─────────────────────────\n",
    "            if client_slvl_data and client_slvr_data and not reference_slv_df.empty:\n",
    "                # Build name→value dicts so we don't rely on fragile positional indexes\n",
    "                slv_cols = ['JH_IN', 'PP_FORCEPLATE', 'PP_W_per_kg', 'Force_at_PP', 'Vel_at_PP']\n",
    "                client_slvl = dict(zip(slv_cols, map(float, client_slvl_data)))\n",
    "                client_slvr = dict(zip(slv_cols, map(float, client_slvr_data)))\n",
    "            \n",
    "                # We want JH, PP, Force@PP (fixed), and Vel@PP\n",
    "                for var in ['JH_IN', 'PP_FORCEPLATE', 'Force_at_PP', 'Vel_at_PP']:\n",
    "                    doc.add_paragraph(f\"{var.replace('_',' ')} Comparison (Left vs Right)\",\n",
    "                                      style='Heading 2')\n",
    "            \n",
    "                    # Reference distribution by column name\n",
    "                    ref_col = reference_slv_df[var].to_numpy(dtype=float, copy=False)\n",
    "            \n",
    "                    # Client values by column name (correct variables now)\n",
    "                    left_val  = client_slvl.get(var, np.nan)\n",
    "                    right_val = client_slvr.get(var, np.nan)\n",
    "            \n",
    "                    hist = generate_slv_histogram(\n",
    "                        var,\n",
    "                        left_val,            # Left\n",
    "                        right_val,           # Right\n",
    "                        ref_col,\n",
    "                        f'{var} Comparison',\n",
    "                        tmpdirname\n",
    "                    )\n",
    "                    doc.add_picture(hist, width=Inches(6))\n",
    "            \n",
    "                # Scatter Force vs Velocity (merge both sides)\n",
    "                client_slv_all = np.array([list(client_slvl.values()),\n",
    "                                           list(client_slvr.values())], dtype=float)\n",
    "                client_slv_dict = {\n",
    "                    'Force_at_PP': [client_slvl['Force_at_PP'], client_slvr['Force_at_PP']],\n",
    "                    'Vel_at_PP'  : [client_slvl['Vel_at_PP'],   client_slvr['Vel_at_PP']],\n",
    "                }\n",
    "                ref_for_scatter = reference_slv_df[['Force_at_PP', 'Vel_at_PP']]\n",
    "                doc.add_paragraph(\"Force vs. Velocity Scatter Plot\", style='Heading 2')\n",
    "                slv_scatter = generate_scatter_plot(\n",
    "                    client_slv_dict,\n",
    "                    ref_for_scatter,\n",
    "                    'Force_at_PP', 'Vel_at_PP',\n",
    "                    'SLV: Force vs. Velocity', tmpdirname\n",
    "                )\n",
    "                doc.add_picture(slv_scatter, width=Inches(6))\n",
    "            else:\n",
    "                if not (client_slvl_data and client_slvr_data):\n",
    "                    print(\"⚠️  Missing SLV client data – skipping SLV graphs.\")\n",
    "                if reference_slv_df.empty:\n",
    "                    print(\"⚠️  SLV reference pull returned 0 rows.\")\n",
    "    \n",
    "        # elif movement == 'NMT':\n",
    "        #     # Fetch NMT data for the client (10s taps only)\n",
    "        #     print(\"Entering NMT section…\")\n",
    "        #     client_cursor.execute(\"SELECT NUM_TAPS_10s FROM NMT WHERE name = ?\", (client_name,))\n",
    "        #     client_nmt_data = client_cursor.fetchone()\n",
    "        # \n",
    "        #     reference_cursor.execute(\"SELECT NUM_TAPS_10s FROM NMT\")\n",
    "        #     reference_nmt_data = np.array(reference_cursor.fetchall(), dtype=float)\n",
    "        # \n",
    "        #     print(f\"NMT client row present: {client_nmt_data is not None}, reference rows: {len(reference_nmt_data)}\")\n",
    "        # \n",
    "        #     if client_nmt_data and reference_nmt_data.size > 0:\n",
    "        #         nmt_var_label = 'NUM TAPS (10s)'\n",
    "        #         doc.add_paragraph(f\"{nmt_var_label} Comparison\", style='Heading 2')\n",
    "        # \n",
    "        #         # Pass table_hint=\"NMT\" to avoid mis-inference\n",
    "        #         nmt_image = generate_bar_graph(\n",
    "        #             'NUM_TAPS_10s',\n",
    "        #             float(client_nmt_data[0]),\n",
    "        #             reference_nmt_data[:, 0],\n",
    "        #             f'{nmt_var_label} Comparison',\n",
    "        #             tmpdirname          # ← stop here\n",
    "        #         )\n",
    "        #         doc.add_picture(nmt_image, width=Inches(6))\n",
    "        #     else:\n",
    "        #         print(\"NMT: missing client or reference data; skipping figure.\")\n",
    "\n",
    "\n",
    "# Function to convert DOCX to images\n",
    "def docx_to_images(docx_path, output_dir):\n",
    "    # Extract text from the DOCX file\n",
    "    text = docx2txt.process(docx_path)\n",
    "    \n",
    "    # Split the text into lines\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Create a blank image with white background\n",
    "    img_width, img_height = 1000, 1500\n",
    "    image = Image.new('RGB', (img_width, img_height), color='white')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Use a simple font\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Draw the text onto the image\n",
    "    padding = 20\n",
    "    y_text = padding\n",
    "    for line in lines:\n",
    "        if y_text + padding > img_height:\n",
    "            # Save the image and start a new one if the text exceeds the page height\n",
    "            img_path = os.path.join(output_dir, f\"page_{int(y_text / img_height)}.png\")\n",
    "            image.save(img_path)\n",
    "            y_text = padding\n",
    "            image = Image.new('RGB', (img_width, img_height), color='white')\n",
    "            draw = ImageDraw.Draw(image)\n",
    "\n",
    "        # Calculate text size and draw it\n",
    "        text_bbox = draw.textbbox((0, 0), line, font=font)\n",
    "        text_width = text_bbox[2] - text_bbox[0]\n",
    "        text_height = text_bbox[3] - text_bbox[1]\n",
    "\n",
    "        draw.text((padding, y_text), line, font=font, fill=\"black\")\n",
    "        y_text += text_height + padding\n",
    "\n",
    "    # Save the last image\n",
    "    img_path = os.path.join(output_dir, \"final_page.png\")\n",
    "    image.save(img_path)\n",
    "\n",
    "    return img_path\n",
    "\n",
    "doc.save(output_filename)          # ← only one final save\n",
    "print(f\"Document saved at: {output_filename}\")\n",
    "\n",
    "# Close connections\n",
    "client_conn.close()\n",
    "reference_conn.close()\n",
    "\n",
    "# Example usage\n",
    "img_output_directory = r'G:\\My Drive\\Athletic Screen 2.0 Reports\\College Reports\\Images'\n",
    "os.makedirs(img_output_directory, exist_ok=True)\n",
    "\n",
    "# Convert DOCX to images\n",
    "img_path = docx_to_images(output_filename, img_output_directory)\n",
    "print(f\"Images saved at {img_path}\")"
   ],
   "id": "88cef313dc0c2b36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases opened successfully.\n",
      "Client Name: Jordan Driver\n",
      "DJ reference rows: 73\n",
      "SLV reference rows: 100\n",
      "Document saved at: G:\\My Drive\\Athletic Screen 2.0 Reports\\College Reports\\Athletic_Report_Jordan Driver_2025-11-07.docx\n",
      "Images saved at G:\\My Drive\\Athletic Screen 2.0 Reports\\College Reports\\Images\\final_page.png\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T21:07:57.054792Z",
     "start_time": "2025-11-07T21:07:42.622651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Creates full report for All group comparison\n",
    "\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from docx import Document\n",
    "from docx.shared import Inches\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "from datetime import date\n",
    "import tempfile\n",
    "import docx2txt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os, re\n",
    "from scipy import integrate, stats\n",
    "import glob as globmod\n",
    "\n",
    "# -------- style to match your dark report --------\n",
    "plt.rcParams.update({\n",
    "    \"figure.facecolor\": \"#181818\",\n",
    "    \"axes.facecolor\"  : \"#303030\",\n",
    "    \"axes.edgecolor\"  : \"white\",\n",
    "    \"axes.labelcolor\" : \"slategrey\",\n",
    "    \"xtick.color\"     : \"lightgrey\",\n",
    "    \"ytick.color\"     : \"lightgrey\",\n",
    "    \"grid.color\"      : \"dimgrey\",\n",
    "    \"text.color\"      : \"white\",\n",
    "})\n",
    "# Corrected file paths with raw strings to handle backslashes properly\n",
    "client_db_path = r'D:\\Athletic Screen 2.0\\Output Files\\movement_database_v2.db'\n",
    "reference_db_path = r'D:\\Athletic Screen 2.0\\Output Files\\Athletic_Screen_All_data_v2.db'\n",
    "\n",
    "# Ensure the paths are valid and accessible\n",
    "if not os.path.exists(client_db_path):\n",
    "    print(f\"Client database not found at {client_db_path}\")\n",
    "if not os.path.exists(reference_db_path):\n",
    "    print(f\"Reference database not found at {reference_db_path}\")\n",
    "\n",
    "# Connect to the client and reference databases\n",
    "client_conn = sqlite3.connect(client_db_path)\n",
    "reference_conn = sqlite3.connect(reference_db_path)\n",
    "client_cursor = client_conn.cursor()\n",
    "reference_cursor = reference_conn.cursor()\n",
    "\n",
    "print(\"Databases opened successfully.\")\n",
    "\n",
    "# Fetch the client's name from the database (assuming the 'name' column is in all tables)\n",
    "client_cursor.execute(\"SELECT DISTINCT name FROM CMJ\")  # Change table if necessary\n",
    "client_name = client_cursor.fetchone()[0]  # Get the first row and first column\n",
    "print(f\"Client Name: {client_name}\")\n",
    "\n",
    "# ---------- build unique export paths (date-stamped, no overwrite) -----\n",
    "client_cursor.execute(\"SELECT MAX(date) FROM CMJ WHERE name = ?\", (client_name,))\n",
    "assessment_date = client_cursor.fetchone()[0]          # e.g. '2025-05-22'\n",
    "if not assessment_date:                                # fallback to today\n",
    "    assessment_date = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "reports_dir = r'G:\\My Drive\\Athletic Screen 2.0 Reports\\College Reports'\n",
    "os.makedirs(reports_dir, exist_ok=True)\n",
    "\n",
    "parts = client_name.split(', ')\n",
    "client_name_rev = f\"{parts[1]}_{parts[0]}\" if len(parts) == 2 else client_name\n",
    "base_name = f\"Athletic_Report_{client_name_rev}_{assessment_date}_All\"\n",
    "\n",
    "output_filename = os.path.join(reports_dir, base_name + \".docx\")\n",
    "img_output_directory = os.path.join(reports_dir, \"Images\", base_name)\n",
    "os.makedirs(img_output_directory, exist_ok=True)\n",
    "\n",
    "# auto-increment if the same file already exists\n",
    "counter = 1\n",
    "while os.path.exists(output_filename):\n",
    "    output_filename = os.path.join(\n",
    "        reports_dir, f\"{base_name}_{counter}.docx\"\n",
    "    )\n",
    "    img_output_directory = os.path.join(\n",
    "        reports_dir, \"Images\", f\"{base_name}_{counter}\"\n",
    "    )\n",
    "    os.makedirs(img_output_directory, exist_ok=True)\n",
    "    counter += 1\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Helper function to calculate percentile\n",
    "def calculate_percentile(value, reference_data):\n",
    "    return stats.percentileofscore(reference_data, value)\n",
    "\n",
    "# ─── UPDATED generate_bar_graph ────────────────────────────────────────────────\n",
    "def generate_bar_graph(variable, client_value, reference_data, title, tmpdirname):\n",
    "    \"\"\"\n",
    "    Blue bars  = reference distribution\n",
    "    ─ red      = client MAX (best trial **within the same movement table**)\n",
    "    ─ violet   = client MEAN (average of those trials)\n",
    "\n",
    "    • No code outside this function needs to change.\n",
    "    • If the caller still passes one score, that is fine; this function looks\n",
    "      up any matching trials on the same assessment day and combines them.\n",
    "    • RSI histograms use 0.25-wide bins so you see bars at 1.25, 1.50, 1.75 …\n",
    "    \"\"\"\n",
    "    import numpy as np, matplotlib.pyplot as plt, os, sqlite3, re\n",
    "\n",
    "    # ───────────── lookup: which movement table are we dealing with? ─────────\n",
    "    column_table_map = {\"CMJ\": [\"CMJ\"], \"DJ\": [\"DJ\"], \"PPU\": [\"PPU\"], \"SLV\": [\"SLV\"], \"NMT\": [\"NMT\"]}\n",
    "    table_guess = None\n",
    "    for tbl in column_table_map:          # CMJ, DJ, SLV, NMT\n",
    "        try:\n",
    "            reference_cursor.execute(f\"SELECT COUNT(*) FROM {tbl}\")\n",
    "            if reference_cursor.fetchone()[0] == len(reference_data):\n",
    "                table_guess = tbl\n",
    "                break\n",
    "        except sqlite3.OperationalError:\n",
    "            continue\n",
    "    if table_guess is None:               # fallback\n",
    "        for tbl in column_table_map:\n",
    "            try:\n",
    "                client_cursor.execute(f\"SELECT 1 FROM {tbl} LIMIT 1\")\n",
    "                table_guess = tbl\n",
    "                break\n",
    "            except sqlite3.OperationalError:\n",
    "                continue\n",
    "\n",
    "    # ───────────── gather all trials for this athlete / table / day ──────────\n",
    "    scores = []\n",
    "    if table_guess:\n",
    "        # Some tables (e.g., NMT) may not have trial_name. If so, fall back safely.\n",
    "        try:\n",
    "            client_cursor.execute(\n",
    "                f\"SELECT trial_name FROM {table_guess} WHERE name=? LIMIT 1\",\n",
    "                (client_name,),\n",
    "            )\n",
    "            row = client_cursor.fetchone()\n",
    "            date_prefix = None\n",
    "            if row and row[0]:\n",
    "                m = re.match(r\"(\\d{4}[-_]\\d{2}[-_]\\d{2})\", row[0])\n",
    "                date_prefix = m.group(1) if m else None\n",
    "    \n",
    "            if date_prefix:\n",
    "                q = f\"SELECT {variable} FROM {table_guess} WHERE name=? AND trial_name LIKE ?\"\n",
    "                client_cursor.execute(q, (client_name, f\"{date_prefix}%\"))\n",
    "            else:\n",
    "                q = f\"SELECT {variable} FROM {table_guess} WHERE name=?\"\n",
    "                client_cursor.execute(q, (client_name,))\n",
    "    \n",
    "            scores = [r[0] for r in client_cursor.fetchall() if r[0] is not None]\n",
    "        except sqlite3.OperationalError:\n",
    "            # No trial_name column (likely NMT) → just use the single value\n",
    "            scores = [client_value]\n",
    "    \n",
    "    # fallback if still empty\n",
    "    if not scores:\n",
    "        scores = [client_value]\n",
    "\n",
    "\n",
    "    scores = np.asarray(scores, dtype=float)\n",
    "    c_mean = scores.mean()\n",
    "    \n",
    "    # --- choose the extreme we draw as the red line ---------------------------\n",
    "    if variable.upper() == \"CT\":        # Contact-Time → use the LOWEST value\n",
    "        c_extreme = scores.min()\n",
    "        extreme_label = \"Client Min\"\n",
    "    else:                               # every other metric → highest value\n",
    "        c_extreme = scores.max()\n",
    "        extreme_label = \"Client Max\"\n",
    "\n",
    "    perc_mean = calculate_percentile(c_mean, reference_data)\n",
    "\n",
    "    # ──────────────────────────── plotting begins ────────────────────────────\n",
    "    plt.figure(facecolor=\"#181818\")\n",
    "    ax = plt.subplot(111, facecolor=\"#303030\")\n",
    "\n",
    "    reference_plotted = False  # flag to avoid double-plotting\n",
    "\n",
    "    # ---------- RSI special case: 0.25-wide bars & custom ticks -------------\n",
    "    if variable.upper() == \"RSI\":\n",
    "        lo = np.floor(reference_data.min() / 0.25) * 0.25\n",
    "        hi = np.ceil(reference_data.max() / 0.25) * 0.25\n",
    "        bins = np.arange(lo, hi + 0.25, 0.25)      # bin edges\n",
    "        centers = bins[:-1]                         # bar positions\n",
    "\n",
    "        counts, _ = np.histogram(reference_data, bins=bins)\n",
    "        ax.bar(\n",
    "            centers,\n",
    "            counts,\n",
    "            width=0.25,\n",
    "            align=\"edge\",\n",
    "            color=\"cornflowerblue\",\n",
    "            alpha=0.7,\n",
    "            edgecolor=\"white\",\n",
    "            label=\"Reference\",\n",
    "        )\n",
    "        # after counts, _ = np.histogram(...)\n",
    "        for x, h in zip(centers, counts):\n",
    "            if h == 0:                        # empty bin → draw thin outline\n",
    "                ax.bar(x, 1e-6, width=.25, align='edge',\n",
    "                       color='none', edgecolor='#404040', linewidth=.5)\n",
    "\n",
    "        ax.set_xticks(centers)\n",
    "        ax.set_xticklabels([f\"{x:.2f}\" for x in centers], color=\"lightgrey\")\n",
    "\n",
    "        reference_plotted = True  # we already drew the reference bars\n",
    "    else:\n",
    "        bins = 20  # default bin count\n",
    "\n",
    "    # ---------- draw reference histogram when not plotted above -------------\n",
    "    if not reference_plotted:\n",
    "        ax.hist(\n",
    "            reference_data,\n",
    "            bins=bins,\n",
    "            color=\"cornflowerblue\",\n",
    "            alpha=0.7,\n",
    "            edgecolor=\"white\",\n",
    "            label=\"Reference\",\n",
    "        )\n",
    "\n",
    "    # ---------- client mean / max lines -------------------------------------\n",
    "    ax.axvline(c_extreme, color=\"red\", ls=\"--\", lw=2, label=\"Client Max\")\n",
    "    ax.axvline(c_mean, color=\"violet\", ls=\"--\", lw=2, label=\"Client Mean\")\n",
    "\n",
    "    # ---------- cosmetics ----------------------------------------------------\n",
    "    ax.set_xlabel(variable.replace(\"_\", \" \"), color=\"slategrey\")\n",
    "    ax.set_ylabel(\"Frequency\", color=\"slategrey\")\n",
    "    ax.tick_params(axis=\"x\", colors=\"lightgrey\")\n",
    "    ax.tick_params(axis=\"y\", colors=\"lightgrey\")\n",
    "    ax.grid(color=\"dimgrey\")\n",
    "\n",
    "    txt = (\n",
    "        f\"Percentile (mean): {perc_mean:.1f}%\"\n",
    "        f\"\\nMean: {c_mean:.2f}\"\n",
    "        f\"\\nMax:  {c_extreme:.2f}\"\n",
    "    )\n",
    "    plt.text(\n",
    "        0.95,\n",
    "        0.05,\n",
    "        txt,\n",
    "        ha=\"right\",\n",
    "        va=\"bottom\",\n",
    "        transform=ax.transAxes,\n",
    "        color=\"white\",\n",
    "        fontsize=9,\n",
    "        backgroundcolor=\"#181818\",\n",
    "    )\n",
    "\n",
    "    ax.legend(facecolor=\"black\", edgecolor=\"grey\", prop={\"size\": \"small\"}, labelcolor=\"grey\")\n",
    "\n",
    "    # ---------- save ---------------------------------------------------------\n",
    "    out_path = os.path.join(tmpdirname, f\"{variable}_histogram.png\")\n",
    "    plt.savefig(out_path, bbox_inches=\"tight\", facecolor=\"#181818\")\n",
    "    plt.close()\n",
    "    return out_path\n",
    "\n",
    "# Function to generate scatter plot for CMJ\n",
    "def generate_scatter_plot(client_data, reference_data, x_var, y_var, title, tmpdirname):\n",
    "    plt.figure(facecolor='#181818', figsize=(6, 6))\n",
    "    ax = plt.subplot(111, facecolor='#303030')\n",
    "\n",
    "    # Create scatter plot for reference data (cornflower blue)\n",
    "    ax.scatter(reference_data[x_var], reference_data[y_var], label='Reference', alpha=0.5, color='cornflowerblue')\n",
    "\n",
    "    # Create scatter plot for client data (red)\n",
    "    ax.scatter(client_data[x_var], client_data[y_var], label='Client', color='red', edgecolors='black', s=100)\n",
    "\n",
    "    # Set axis labels, replacing underscores with spaces\n",
    "    ax.set_xlabel(x_var.replace('_', ' '), color='slategrey')\n",
    "    ax.set_ylabel(y_var.replace('_', ' '), color='slategrey')\n",
    "\n",
    "    # Dynamically set ticks and numbers to light grey\n",
    "    ax.tick_params(axis='x', colors='lightgrey')\n",
    "    ax.tick_params(axis='y', colors='lightgrey')\n",
    "\n",
    "    # Add vertical and horizontal reference lines (light grey)\n",
    "    ax.axvline(x=np.mean(reference_data[x_var]), color='lightgrey', linestyle='--', linewidth=1)\n",
    "    ax.axhline(y=np.mean(reference_data[y_var]), color='lightgrey', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Customize grid style\n",
    "    ax.grid(color='dimgrey')\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(facecolor='black', edgecolor='grey', prop={'size': 'small'}, labelcolor='grey')\n",
    "\n",
    "    # Save scatter plot to file\n",
    "    scatter_filename = os.path.join(tmpdirname, 'cmj_scatter.png')\n",
    "    plt.savefig(scatter_filename, bbox_inches='tight', facecolor='#181818')\n",
    "    plt.close()\n",
    "\n",
    "    return scatter_filename\n",
    "\n",
    "\n",
    "def load_power_txt(txt_path: str) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Parse exported power file (like your example). Returns a pandas Series of power.\n",
    "    Assumes: header lines, then a line starting with 'ITEM', then data rows:\n",
    "             <index>\\t<value>\n",
    "    Skips rows without a numeric second field.\n",
    "    \"\"\"\n",
    "    power_vals = []\n",
    "    in_data = False\n",
    "    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        for raw in f:\n",
    "            line = raw.strip()\n",
    "            if not in_data:\n",
    "                if line.startswith(\"ITEM\"):\n",
    "                    in_data = True\n",
    "                continue\n",
    "            # from here on, try to parse the last field as a float\n",
    "            if not line:\n",
    "                continue\n",
    "            parts = re.split(r\"\\t+\", line)\n",
    "            if len(parts) < 2:\n",
    "                # sometimes a blank power cell exists on the first data row — skip it\n",
    "                continue\n",
    "            try:\n",
    "                val = float(parts[-1])\n",
    "                power_vals.append(val)\n",
    "            except ValueError:\n",
    "                # non-numeric tail → ignore\n",
    "                continue\n",
    "    if not power_vals:\n",
    "        raise ValueError(f\"No power values parsed from {txt_path}\")\n",
    "    return pd.Series(power_vals, name=\"Power\")\n",
    "\n",
    "def analyze_power_curve(power: pd.Series, fs_hz: float = 1000.0) -> dict:\n",
    "    \"\"\"\n",
    "    Compute useful shape/temporal features.\n",
    "    power: Series of power (W). fs_hz: sampling rate (Hz). If unknown, 1000 Hz is typical.\n",
    "    Returns a dict of metrics.\n",
    "    \"\"\"\n",
    "    p = np.asarray(power, dtype=float)\n",
    "    n = p.size\n",
    "    t = np.arange(n) / fs_hz\n",
    "\n",
    "    # basic\n",
    "    p_peak_idx = int(np.nanargmax(p))\n",
    "    p_peak     = float(p[p_peak_idx])\n",
    "    t_peak     = float(t[p_peak_idx])\n",
    "\n",
    "    # onset/offset via % of peak (robust to baseline drift)\n",
    "    thr10 = 0.10 * p_peak\n",
    "    thr50 = 0.50 * p_peak\n",
    "    thr90 = 0.90 * p_peak\n",
    "\n",
    "    # first index above 10% of peak\n",
    "    try:\n",
    "        onset_idx = int(np.argmax(p >= thr10))\n",
    "    except ValueError:\n",
    "        onset_idx = 0\n",
    "    # first index after peak that falls below 10% (or end)\n",
    "    post = p[p_peak_idx:]\n",
    "    off_rel = np.argmax(post < thr10) if np.any(post < thr10) else (post.size - 1)\n",
    "    offset_idx = p_peak_idx + int(off_rel)\n",
    "\n",
    "    # 10–90% rise time on rising limb\n",
    "    rising = p[:p_peak_idx+1]\n",
    "    try:\n",
    "        i10 = int(np.argmax(rising >= thr10))\n",
    "        i90 = int(np.argmax(rising >= thr90))\n",
    "        rise_time = (i90 - i10) / fs_hz if i90 > i10 else np.nan\n",
    "        rise_slope = (0.8 * p_peak) / rise_time if rise_time and rise_time > 0 else np.nan\n",
    "    except ValueError:\n",
    "        i10 = i90 = None\n",
    "        rise_time = np.nan\n",
    "        rise_slope = np.nan\n",
    "\n",
    "    # FWHM (50% of peak) width\n",
    "    # left crossing\n",
    "    try:\n",
    "        left_idx  = int(np.argmax(rising >= thr50))\n",
    "    except ValueError:\n",
    "        left_idx = p_peak_idx\n",
    "    # right crossing\n",
    "    falling = p[p_peak_idx:]\n",
    "    try:\n",
    "        right_rel = int(np.argmax(falling <= thr50))\n",
    "        right_idx = p_peak_idx + right_rel\n",
    "    except ValueError:\n",
    "        right_idx = p_peak_idx\n",
    "    fwhm_sec = (right_idx - left_idx) / fs_hz if right_idx > left_idx else np.nan\n",
    "\n",
    "    # Work/impulse of power (area under curve) over the active window\n",
    "    a = max(0, onset_idx)\n",
    "    b = min(n - 1, max(offset_idx, p_peak_idx))\n",
    "    auc_joules = float(np.trapezoid(np.nan_to_num(p[a:b+1], nan=0.0), dx=1.0/fs_hz))\n",
    "\n",
    "    # timing “balance”: center of mass of power curve (0..1)\n",
    "    # (earlier vs. later power concentration)\n",
    "    weights = p[a:b+1].clip(min=0)\n",
    "    if weights.sum() > 0:\n",
    "        t_window = t[a:b+1]\n",
    "        t_com = float(np.sum(t_window * weights) / np.sum(weights))\n",
    "        t_com_norm = (t_com - t[a]) / max(1e-9, (t[b] - t[a]))\n",
    "    else:\n",
    "        t_com = np.nan\n",
    "        t_com_norm = np.nan\n",
    "\n",
    "    # variability around peak (local coefficient of variation in ±50 ms)\n",
    "    w = int(0.05 * fs_hz)\n",
    "    lo = max(0, p_peak_idx - w)\n",
    "    hi = min(n, p_peak_idx + w + 1)\n",
    "    local = p[lo:hi]\n",
    "    cv_local = float(np.std(local) / np.mean(local)) if np.mean(local) > 0 else np.nan\n",
    "\n",
    "    return {\n",
    "        \"n_samples\": n,\n",
    "        \"fs_hz\": fs_hz,\n",
    "        \"peak_power_w\": p_peak,\n",
    "        \"time_to_peak_s\": t_peak,\n",
    "        \"rise_time_10_90_s\": float(rise_time),\n",
    "        \"rise_slope_w_per_s\": float(rise_slope),\n",
    "        \"fwhm_s\": float(fwhm_sec),\n",
    "        \"auc_j\": auc_joules,\n",
    "        \"onset_idx\": a,\n",
    "        \"offset_idx\": b,\n",
    "        \"peak_idx\": p_peak_idx,\n",
    "        \"t_com_s\": t_com,\n",
    "        \"t_com_norm_0to1\": t_com_norm,\n",
    "        \"cv_local_peak\": cv_local,\n",
    "        \"i10_idx\": int(i10) if isinstance(i10, int) else None,\n",
    "        \"i90_idx\": int(i90) if isinstance(i90, int) else None,\n",
    "        \"left50_idx\": left_idx,\n",
    "        \"right50_idx\": right_idx,\n",
    "    }\n",
    "\n",
    "def plot_power_curve(power: pd.Series,\n",
    "                     metrics: dict,\n",
    "                     out_path: str,\n",
    "                     title: str = \"Power Curve\",\n",
    "                     annotate: bool = True):\n",
    "    \"\"\"\n",
    "    Plot power vs. time with annotations (peak, 10–90 rise, FWHM, AUC window).\n",
    "    \"\"\"\n",
    "    p = np.asarray(power, dtype=float)\n",
    "    t = np.arange(p.size) / metrics[\"fs_hz\"]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 3.6))\n",
    "    ax.plot(t, p, lw=2, label=\"Power\")\n",
    "    ax.grid(True)\n",
    "    ax.set_xlabel(\"Time (s)\")\n",
    "    ax.set_ylabel(\"Power (W)\")\n",
    "    ax.set_title(title, pad=8, color=\"white\")\n",
    "\n",
    "    # annotate regions\n",
    "    a, b = metrics[\"onset_idx\"], metrics[\"offset_idx\"]\n",
    "    ax.axvspan(t[a], t[b], color=\"white\", alpha=0.07, label=\"active window\")\n",
    "\n",
    "    # peak\n",
    "    pk = metrics[\"peak_idx\"]\n",
    "    ax.plot([t[pk]], [p[pk]], \"o\", ms=6, color=\"tomato\", label=\"Peak\")\n",
    "    ax.axhline(p[pk]*0.5, ls=\"--\", lw=1, color=\"grey\")\n",
    "    ax.vlines([t[metrics[\"left50_idx\"]], t[metrics[\"right50_idx\"]]],\n",
    "              ymin=0, ymax=p[pk]*0.5, linestyles=\"--\", colors=\"grey\", lw=1)\n",
    "\n",
    "    # 10–90 rise (if available)\n",
    "    if metrics[\"i10_idx\"] is not None and metrics[\"i90_idx\"] is not None:\n",
    "        ax.plot([t[metrics[\"i10_idx\"]], t[metrics[\"i90_idx\"]]],\n",
    "                [p[metrics[\"i10_idx\"]], p[metrics[\"i90_idx\"]]],\n",
    "                lw=3, color=\"deepskyblue\", label=\"10–90% rise\")\n",
    "\n",
    "    if annotate:\n",
    "        txt = (f\"Peak: {metrics['peak_power_w']:.1f} W @ {metrics['time_to_peak_s']:.3f} s\"\n",
    "               f\"\\nRise 10–90: {metrics['rise_time_10_90_s']:.3f} s\"\n",
    "               f\"\\nFWHM: {metrics['fwhm_s']:.3f} s\"\n",
    "               f\"\\nWork (AUC): {metrics['auc_j']:.1f} J\"\n",
    "               f\"\\nTiming COM: {metrics['t_com_norm_0to1']:.2f} (0 early…1 late)\")\n",
    "        ax.text(0.99, 0.02, txt, ha=\"right\", va=\"bottom\",\n",
    "                transform=ax.transAxes, fontsize=9, color=\"white\",\n",
    "                bbox=dict(boxstyle=\"round,pad=0.25\", facecolor=\"#181818\", edgecolor=\"#444\"))\n",
    "\n",
    "    ax.legend(facecolor=\"black\", edgecolor=\"grey\", prop={\"size\": \"small\"}, labelcolor=\"grey\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def overlay_power_trials(traces: list[pd.Series],\n",
    "                         fs_hz: float,\n",
    "                         out_path: str,\n",
    "                         title: str = \"Power (all trials)\",\n",
    "                         align: str = \"peak\",          # \"peak\", \"onset10\", or \"none\"\n",
    "                         window_s: tuple[float, float] | None = (0.30, 0.40),\n",
    "                         show_mean: bool = True):\n",
    "    \"\"\"\n",
    "    Overlay multiple power traces and align them in time.\n",
    "\n",
    "    align:\n",
    "      - \"peak\"    → align each trial's max power to t=0\n",
    "      - \"onset10\" → align first sample ≥ 10% of that trial's peak to t=0\n",
    "      - \"none\"    → no alignment; left edges at t=0\n",
    "\n",
    "    window_s: (pre, post) seconds to show around t=0 (None to show full extent)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- convert input to arrays; find alignment index per trial --------------\n",
    "    arrs = [np.asarray(s, dtype=float) for s in traces if len(s) > 0]\n",
    "    if not arrs:\n",
    "        raise ValueError(\"overlay_power_trials: no non-empty traces provided\")\n",
    "\n",
    "    def _align_index(x: np.ndarray) -> int:\n",
    "        if align == \"none\":\n",
    "            return 0\n",
    "        # guard against NaNs\n",
    "        if not np.any(np.isfinite(x)):\n",
    "            return 0\n",
    "        # peak index\n",
    "        try:\n",
    "            pk = int(np.nanargmax(x))\n",
    "        except ValueError:\n",
    "            pk = 0\n",
    "        if align == \"peak\":\n",
    "            return pk\n",
    "        elif align == \"onset10\":\n",
    "            thr = 0.10 * (x[pk] if np.isfinite(x[pk]) else np.nanmax(x))\n",
    "            # first index >= 10% of that trial's peak\n",
    "            idx = int(np.argmax(x >= thr)) if np.any(x >= thr) else 0\n",
    "            return idx\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    align_idx = [ _align_index(x) for x in arrs ]\n",
    "\n",
    "    # --- build a padded matrix so all alignment points land at the same column\n",
    "    max_left  = max(align_idx)                                  # largest left padding needed\n",
    "    right_len = [len(x) - i for x, i in zip(arrs, align_idx)]   # samples from align idx to end\n",
    "    max_right = max(right_len)\n",
    "    L = max_left + max_right                                     # total aligned length\n",
    "\n",
    "    aligned = np.full((len(arrs), L), np.nan)\n",
    "    for r, (x, i0) in enumerate(zip(arrs, align_idx)):\n",
    "        start = max_left - i0\n",
    "        aligned[r, start:start+len(x)] = x\n",
    "\n",
    "    # --- time vector: t=0 at the common alignment column ---------------------\n",
    "    t = (np.arange(L) - max_left) / fs_hz\n",
    "\n",
    "    # --- optional cropping around t=0 ----------------------------------------\n",
    "    if window_s is not None:\n",
    "        pre, post = window_s\n",
    "        i_lo = max(0, int(np.floor((-pre)  * fs_hz)) + max_left)\n",
    "        i_hi = min(L, int(np.ceil( (post) * fs_hz)) + max_left)\n",
    "        aligned = aligned[:, i_lo:i_hi]\n",
    "        t = t[i_lo:i_hi]\n",
    "        \n",
    "    # --- plot ----------------------------------------------------------------\n",
    "    fig, ax = plt.subplots(figsize=(6, 3.6))\n",
    "    \n",
    "    # consistent y-limits (same scale across trials)\n",
    "    y_max = np.nanmax(aligned)\n",
    "    y_min = np.nanmin(aligned)\n",
    "    \n",
    "    # NEW: mask-based plotting so left/right NaNs don't truncate the curve visually\n",
    "    coverage_flags = []  # we'll use this to warn about very short traces\n",
    "    for i in range(aligned.shape[0]):\n",
    "        row = aligned[i]\n",
    "        mask = np.isfinite(row)\n",
    "        cov = np.count_nonzero(mask) / max(1, len(row))\n",
    "        coverage_flags.append(cov)\n",
    "        if np.count_nonzero(mask) >= 2:\n",
    "            ax.plot(t[mask], row[mask], lw=1.2, alpha=0.6)\n",
    "    \n",
    "    # optional: highlight which trials are very short (<50% of the window)\n",
    "    short = [idx for idx, c in enumerate(coverage_flags, start=1) if c < 0.5]\n",
    "    if short:\n",
    "        print(f\"⚠️ overlay_power_trials: {len(short)} trial(s) with <50% coverage in the window: {short}. \"\n",
    "              f\"Consider changing window_s or checking the export files.\")\n",
    "    \n",
    "    if show_mean:\n",
    "        mean_curve = np.nanmean(aligned, axis=0)\n",
    "        ax.plot(t, mean_curve, lw=2.2, color=\"cyan\", label=\"Mean\")\n",
    "    \n",
    "    # vertical line at alignment point\n",
    "    ax.axvline(0.0, color=\"grey\", lw=1, ls=\"--\",\n",
    "               label=(\"Aligned peak\" if align == \"peak\" else \"Aligned onset\"))\n",
    "    \n",
    "    ax.grid(True)\n",
    "    ax.set_xlabel(\"Time (s, aligned)\")\n",
    "    ax.set_ylabel(\"Power (W)\")\n",
    "    ax.set_title(title, pad=8, color=\"white\")\n",
    "    if show_mean:\n",
    "        ax.legend(facecolor=\"black\", edgecolor=\"grey\", prop={\"size\": \"small\"}, labelcolor=\"grey\")\n",
    "    \n",
    "    ax.set_ylim(y_min - 0.05 * abs(y_max - y_min), y_max + 0.05 * abs(y_max - y_min))\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(out_path, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "def find_power_files(movement: str,\n",
    "                     base_dir: str = r'D:\\Athletic Screen 2.0\\Output Files') -> list[str]:\n",
    "    \"\"\"\n",
    "    Finds exported power files for a movement. Supports:\n",
    "      CMJ_Power.txt, CMJ1_Power.txt, CMJ2_Power.txt, ...\n",
    "      DJ_Power.txt,  DJ*_Power.txt\n",
    "      SLV_Power.txt, SLV*_Power.txt\n",
    "    Returns a sorted (unique) list of full paths.\n",
    "    \"\"\"\n",
    "    pats = [f\"{movement}_Power.txt\", f\"{movement}*_Power.txt\"]\n",
    "    files = []\n",
    "    for p in pats:\n",
    "        files += globmod.glob(os.path.join(base_dir, p))\n",
    "    return sorted(set(files))\n",
    "\n",
    "def build_aligned_matrix(traces: list[pd.Series],\n",
    "                         fs_hz: float,\n",
    "                         align: str = \"peak\",\n",
    "                         window_s: tuple[float, float] | None = (0.30, 0.40)):\n",
    "    \"\"\"\n",
    "    Returns (aligned 2D array [n_trials x T], time vector [T], indices_info[list]).\n",
    "    Time is centered so the chosen alignment point is at t=0.\n",
    "    \"\"\"\n",
    "    arrs = [np.asarray(s, dtype=float) for s in traces if len(s) > 0]\n",
    "    if not arrs:\n",
    "        raise ValueError(\"build_aligned_matrix: no non-empty traces\")\n",
    "\n",
    "    def _align_idx(x: np.ndarray) -> int:\n",
    "        if align == \"none\":\n",
    "            return 0\n",
    "        if not np.any(np.isfinite(x)):\n",
    "            return 0\n",
    "        pk = int(np.nanargmax(x))\n",
    "        if align == \"peak\":\n",
    "            return pk\n",
    "        elif align == \"onset10\":\n",
    "            thr = 0.10 * (x[pk] if np.isfinite(x[pk]) else np.nanmax(x))\n",
    "            return int(np.argmax(x >= thr)) if np.any(x >= thr) else 0\n",
    "        return 0\n",
    "\n",
    "    aidx = [_align_idx(x) for x in arrs]\n",
    "    max_left = max(aidx)\n",
    "    right_len = [len(x) - i for x, i in zip(arrs, aidx)]\n",
    "    max_right = max(right_len)\n",
    "    L = max_left + max_right\n",
    "\n",
    "    aligned = np.full((len(arrs), L), np.nan)\n",
    "    for r, (x, i0) in enumerate(zip(arrs, aidx)):\n",
    "        start = max_left - i0\n",
    "        aligned[r, start:start+len(x)] = x\n",
    "\n",
    "    t = (np.arange(L) - max_left) / fs_hz\n",
    "\n",
    "    if window_s is not None:\n",
    "        pre, post = window_s\n",
    "        i_lo = max(0, int(np.floor((-pre)  * fs_hz)) + max_left)\n",
    "        i_hi = min(L, int(np.ceil( (post) * fs_hz)) + max_left)\n",
    "        return aligned[:, i_lo:i_hi], t[i_lo:i_hi], aidx\n",
    "    return aligned, t, aidx\n",
    "\n",
    "def mean_aligned_curve(traces: list[pd.Series],\n",
    "                       fs_hz: float,\n",
    "                       align: str = \"peak\",\n",
    "                       window_s: tuple[float, float] | None = (0.30, 0.40)) -> pd.Series:\n",
    "    aligned, t, _ = build_aligned_matrix(traces, fs_hz, align, window_s)\n",
    "    mean_curve = np.nanmean(aligned, axis=0)\n",
    "    return pd.Series(mean_curve, index=t, name=\"Power\")\n",
    "\n",
    "def analyze_power_curve_advanced(power: pd.Series, fs_hz: float = 1000.0) -> dict:\n",
    "    \"\"\"\n",
    "    Extends your analyze_power_curve with additional, practical features:\n",
    "      • rpd_max (max rate of power development) & time to RPDmax\n",
    "      • AUC early (pre-peak) / late (post-peak), % early work\n",
    "      • decay_90_10 on falling limb\n",
    "      • skewness, kurtosis\n",
    "      • spectral centroid (how ‘fast’ the curve is in frequency domain)\n",
    "    \"\"\"\n",
    "    base = analyze_power_curve(power, fs_hz)\n",
    "    p = np.asarray(power, dtype=float)\n",
    "    n = p.size\n",
    "    t = np.arange(n) / fs_hz\n",
    "\n",
    "    # RPD\n",
    "    dp = np.gradient(p, 1.0/fs_hz)\n",
    "    rpd_max = float(np.nanmax(dp))\n",
    "    rpd_idx = int(np.nanargmax(dp))\n",
    "    base[\"rpd_max_w_per_s\"]   = rpd_max\n",
    "    base[\"time_to_rpd_max_s\"] = rpd_idx / fs_hz\n",
    "\n",
    "    # Early/late work around peak (use on/peak/off from base)\n",
    "    a, b, pk = base[\"onset_idx\"], base[\"offset_idx\"], base[\"peak_idx\"]\n",
    "    auc_pre  = float(np.trapezoid(np.nan_to_num(p[a:pk+1],  nan=0.0), dx=1.0/fs_hz)) if pk >= a else np.nan\n",
    "    auc_post = float(np.trapezoid(np.nan_to_num(p[pk:b+1], nan=0.0), dx=1.0/fs_hz)) if b >= pk else np.nan\n",
    "    total    = (auc_pre if np.isfinite(auc_pre) else 0) + (auc_post if np.isfinite(auc_post) else 0)\n",
    "    base[\"auc_pre_j\"]      = auc_pre\n",
    "    base[\"auc_post_j\"]     = auc_post\n",
    "    base[\"work_early_pct\"] = float(100.0 * auc_pre / total) if total > 0 else np.nan\n",
    "\n",
    "    # Decay time 90→10% of peak on falling limb\n",
    "    peak_val = p[pk]\n",
    "    fall = p[pk:]\n",
    "    thr90 = 0.90 * peak_val\n",
    "    thr10 = 0.10 * peak_val\n",
    "    i90 = int(np.argmax(fall <= thr90)) if np.any(fall <= thr90) else 0\n",
    "    i10 = int(np.argmax(fall <= thr10)) if np.any(fall <= thr10) else len(fall)-1\n",
    "    base[\"decay_90_10_s\"] = (i10 - i90) / fs_hz if i10 > i90 else np.nan\n",
    "\n",
    "    # Shape stats\n",
    "    finite = np.isfinite(p)\n",
    "    base[\"skewness\"] = float(stats.skew(p[finite])) if np.any(finite) else np.nan\n",
    "    base[\"kurtosis\"] = float(stats.kurtosis(p[finite], fisher=True)) if np.any(finite) else np.nan\n",
    "\n",
    "    # Spectral centroid\n",
    "    x = p - np.nanmean(p)\n",
    "    X = np.abs(np.fft.rfft(np.nan_to_num(x)))\n",
    "    freqs = np.fft.rfftfreq(x.size, d=1.0/fs_hz)\n",
    "    base[\"spectral_centroid_hz\"] = float(np.sum(freqs * X) / max(1e-12, np.sum(X)))\n",
    "\n",
    "    return base\n",
    "\n",
    "def add_power_analysis_section(doc: Document,\n",
    "                               movement: str,\n",
    "                               traces: list[pd.Series],\n",
    "                               fs_hz: float,\n",
    "                               tmpdirname: str,\n",
    "                               reference_cursor,\n",
    "                               reference_table: str):\n",
    "    \"\"\"\n",
    "    1) Overlay (peaks aligned) + mean curve\n",
    "    2) Annotated mean power curve\n",
    "    3) Table of per-trial metrics + Mean/SD\n",
    "    4) Adds reference percentile for peak power (vs PP_FORCEPLATE in ref DB)\n",
    "    \"\"\"\n",
    "    # --- 1) Overlay (aligned at peak) ---------------------------------------\n",
    "    overlay_png = os.path.join(tmpdirname, f\"{movement}_power_overlay.png\")\n",
    "    overlay_power_trials(\n",
    "        traces, fs_hz=fs_hz, out_path=overlay_png,\n",
    "        title=f\"{movement} Power – All Trials (peaks aligned)\",\n",
    "        align=\"peak\", window_s=(0.30, 0.40), show_mean=True\n",
    "    )\n",
    "    doc.add_paragraph(\"Power Curves (aligned at peak)\", style=\"Heading 2\")\n",
    "    doc.add_picture(overlay_png, width=Inches(6))\n",
    "\n",
    "    # --- 2) Mean curve (aligned) + annotated plot ---------------------------\n",
    "    mean_series = mean_aligned_curve(traces, fs_hz, align=\"peak\", window_s=(0.30, 0.40))\n",
    "    mean_metrics = analyze_power_curve_advanced(mean_series, fs_hz=fs_hz)\n",
    "\n",
    "    mean_png = os.path.join(tmpdirname, f\"{movement}_power_mean_annotated.png\")\n",
    "    plot_power_curve(mean_series, mean_metrics, mean_png,\n",
    "                     title=f\"{movement} – Mean Power (aligned)\")\n",
    "    doc.add_paragraph(\"Mean Power Curve (annotated)\", style=\"Heading 3\")\n",
    "    doc.add_picture(mean_png, width=Inches(6))\n",
    "\n",
    "    # --- 3) Per-trial metrics & summary -------------------------------------\n",
    "    per = [analyze_power_curve_advanced(s, fs_hz=fs_hz) for s in traces]\n",
    "    df  = pd.DataFrame(per)\n",
    "\n",
    "    # Pick the important rows for the document (order here = row order in table)\n",
    "    metric_rows = [\n",
    "        (\"Peak Power (W)\",            \"peak_power_w\",        \"{:.0f}\"),\n",
    "        (\"Time to Peak (s)\",          \"time_to_peak_s\",      \"{:.3f}\"),\n",
    "        (\"RPD max (W/s)\",             \"rpd_max_w_per_s\",     \"{:.0f}\"),\n",
    "        (\"Time to RPD max (s)\",       \"time_to_rpd_max_s\",   \"{:.3f}\"),\n",
    "        (\"Rise 10–90% (s)\",           \"rise_time_10_90_s\",   \"{:.3f}\"),\n",
    "        (\"FWHM (s)\",                  \"fwhm_s\",              \"{:.3f}\"),\n",
    "        (\"Work (AUC, J)\",             \"auc_j\",               \"{:.0f}\"),\n",
    "        (\"Early work (%)\",            \"work_early_pct\",      \"{:.1f}\"),\n",
    "        (\"Decay 90→10% (s)\",          \"decay_90_10_s\",       \"{:.3f}\"),\n",
    "        (\"Timing CoM (0…1)\",          \"t_com_norm_0to1\",     \"{:.2f}\"),\n",
    "        (\"Skewness\",                  \"skewness\",            \"{:.2f}\"),\n",
    "        (\"Kurtosis\",                  \"kurtosis\",            \"{:.2f}\"),\n",
    "        (\"Spectral centroid (Hz)\",    \"spectral_centroid_hz\",\"{:.2f}\"),\n",
    "    ]\n",
    "\n",
    "    # table: Metric | Trial1 | Trial2 | ... | Mean | SD\n",
    "    tbl = doc.add_table(rows=1 + len(metric_rows), cols=2 + len(traces))\n",
    "    tbl.style = \"Light List\" if \"Light List\" in [s.name for s in doc.styles] else tbl.style\n",
    "    # header\n",
    "    hdr = tbl.rows[0].cells\n",
    "    hdr[0].text = \"Metric\"\n",
    "    for i in range(len(traces)):\n",
    "        hdr[1+i].text = f\"Trial {i+1}\"\n",
    "    hdr[-1].text = \"Mean ± SD\"\n",
    "\n",
    "    # body\n",
    "    for r, (label, key, fmt) in enumerate(metric_rows, start=1):\n",
    "        row_cells = tbl.rows[r].cells\n",
    "        row_cells[0].text = label\n",
    "        vals = df.get(key, pd.Series([np.nan]*len(traces))).values\n",
    "        for i, v in enumerate(vals):\n",
    "            row_cells[1+i].text = (fmt.format(v) if np.isfinite(v) else \"—\")\n",
    "        mu = np.nanmean(vals)\n",
    "        sd = np.nanstd(vals, ddof=1) if np.count_nonzero(np.isfinite(vals)) > 1 else np.nan\n",
    "        row_cells[-1].text = (f\"{fmt.format(mu)} ± {fmt.format(sd)}\"\n",
    "                              if np.isfinite(mu) and np.isfinite(sd) else\n",
    "                              (fmt.format(mu) if np.isfinite(mu) else \"—\"))\n",
    "\n",
    "    # small spacer\n",
    "    doc.add_paragraph(\"\")\n",
    "\n",
    "    # --- 4) Reference percentile for peak power (vs. DB PP_FORCEPLATE) ------\n",
    "    try:\n",
    "        reference_cursor.execute(f\"SELECT PP_FORCEPLATE FROM {reference_table} WHERE PP_FORCEPLATE IS NOT NULL\")\n",
    "        ref_pp = np.array([r[0] for r in reference_cursor.fetchall()], dtype=float)\n",
    "    except sqlite3.OperationalError:\n",
    "        ref_pp = np.array([])\n",
    "\n",
    "    if ref_pp.size:\n",
    "        # use the highest peak among trials (or mean peak if you prefer)\n",
    "        trial_peaks = df[\"peak_power_w\"].values\n",
    "        best_peak   = float(np.nanmax(trial_peaks)) if trial_peaks.size else np.nan\n",
    "        pctl        = percentile_vs_reference(best_peak, ref_pp) if np.isfinite(best_peak) else np.nan\n",
    "        doc.add_paragraph(f\"Reference percentile (peak power): {pctl:.1f}%\", style=\"Intense Quote\")\n",
    "\n",
    "def percentile_vs_reference(value: float, reference_values: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Wrapper for percentile (1..99) using scipy-like methodology.\n",
    "    \"\"\"\n",
    "    return stats.percentileofscore(reference_values, value)\n",
    "\n",
    "# Modified function to generate a histogram comparing left and right leg data\n",
    "def generate_slv_histogram(variable, left_value, right_value,\n",
    "                           reference_data, title, tmpdirname):\n",
    "    \"\"\"\n",
    "    Blue bars  = reference distribution\n",
    "    ─ green    = client LEFT (latest trial value you passed in)\n",
    "    ─ orange   = client RIGHT\n",
    "\n",
    "    Text box shows, for each side:\n",
    "        • mean across all trials from the same assessment day\n",
    "        • max across those trials\n",
    "        • percentile of that mean vs. the reference distribution\n",
    "    \"\"\"\n",
    "    import numpy as np, matplotlib.pyplot as plt, os, re, sqlite3\n",
    "\n",
    "    # ── helper – get all trials for a given side on the same assessment day ──\n",
    "    def _fetch_side_vals(side):\n",
    "        # detect yyyy-mm-dd prefix in the first trial_name for this side\n",
    "        client_cursor.execute(\n",
    "            \"SELECT trial_name FROM SLV WHERE name=? AND side=? LIMIT 1\",\n",
    "            (client_name, side)\n",
    "        )\n",
    "        row = client_cursor.fetchone()\n",
    "        date_prefix = None\n",
    "        if row and row[0]:\n",
    "            m = re.match(r'(\\d{4}[-_]\\d{2}[-_]\\d{2})', row[0])\n",
    "            date_prefix = m.group(1) if m else None\n",
    "\n",
    "        if date_prefix:\n",
    "            q = f\"SELECT {variable} FROM SLV WHERE name=? AND side=? AND trial_name LIKE ?\"\n",
    "            client_cursor.execute(q, (client_name, side, f'{date_prefix}%'))\n",
    "        else:\n",
    "            q = f\"SELECT {variable} FROM SLV WHERE name=? AND side=?\"\n",
    "            client_cursor.execute(q, (client_name, side))\n",
    "\n",
    "        return [r[0] for r in client_cursor.fetchall() if r[0] is not None]\n",
    "\n",
    "    # pull all trials for each side; fall back to the single value passed in\n",
    "    left_vals  = np.asarray(_fetch_side_vals('Left')  or [left_value],  dtype=float)\n",
    "    right_vals = np.asarray(_fetch_side_vals('Right') or [right_value], dtype=float)\n",
    "\n",
    "    left_mean,  left_max  = left_vals.mean(),  left_vals.max()\n",
    "    right_mean, right_max = right_vals.mean(), right_vals.max()\n",
    "\n",
    "    left_pct  = calculate_percentile(left_mean,  reference_data)\n",
    "    right_pct = calculate_percentile(right_mean, reference_data)\n",
    "\n",
    "    # ── plot ────────────────────────────────────────────────────────────────\n",
    "    plt.figure(facecolor='#181818')\n",
    "    ax = plt.subplot(111, facecolor='#303030')\n",
    "\n",
    "    ax.hist(reference_data, bins=20, color='cornflowerblue',\n",
    "            alpha=0.7, edgecolor='white', label='Reference')\n",
    "\n",
    "    ax.axvline(left_value,  color='green',  ls='--', lw=2, label='Left (latest)')\n",
    "    ax.axvline(right_value, color='orange', ls='--', lw=2, label='Right (latest)')\n",
    "\n",
    "    ax.set_xlabel(variable.replace('_', ' '), color='slategrey')\n",
    "    ax.set_ylabel('Frequency',               color='slategrey')\n",
    "    ax.tick_params(axis='x', colors='lightgrey')\n",
    "    ax.tick_params(axis='y', colors='lightgrey')\n",
    "    ax.grid(color='dimgrey')\n",
    "\n",
    "    txt = (\n",
    "        f'LEFT  – mean: {left_mean:.2f}\\n'\n",
    "        f'        max:  {left_max:.2f}\\n'\n",
    "        f'        %ile: {left_pct:.1f}\\n'\n",
    "        f'RIGHT – mean: {right_mean:.2f}\\n'\n",
    "        f'        max:  {right_max:.2f}\\n'\n",
    "        f'        %ile: {right_pct:.1f}'\n",
    "    )\n",
    "    plt.text(0.95, 0.05, txt, ha='right', va='bottom',\n",
    "             transform=ax.transAxes, color='white', fontsize=9,\n",
    "             backgroundcolor='#181818')\n",
    "\n",
    "    ax.legend(facecolor='black', edgecolor='grey',\n",
    "              prop={'size': 'small'}, labelcolor='grey')\n",
    "\n",
    "    out_path = os.path.join(tmpdirname, f'{variable}_histogram_slv.png')\n",
    "    plt.savefig(out_path, bbox_inches='tight', facecolor='#181818')\n",
    "    plt.close()\n",
    "    return out_path\n",
    "\n",
    "# Prepare the document\n",
    "doc = Document()\n",
    "doc.add_picture(\"8ctane Baseball - Black abd Blue BG.jpeg\", width=Inches(4.0))  # Replace with your logo path\n",
    "doc.paragraphs[-1].alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "\n",
    "# Adding player name and date\n",
    "doc.add_paragraph(f\"Player's Name: {client_name}\")  # Replace client_name with dynamic value\n",
    "doc.add_paragraph(f\"Date: {date.today().strftime('%B %d, %Y')}\")\n",
    "\n",
    "# Create a temporary directory to store images\n",
    "with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "    # List of movements to process\n",
    "    movements = ['CMJ', 'DJ', 'PPU', 'SLV', 'NMT']\n",
    "    \n",
    "    for movement in movements:\n",
    "        # Add movement title\n",
    "        doc.add_paragraph(f\"{movement} Report\", style='Title')\n",
    "        doc.add_paragraph(f\"This section includes percentile reports and comparisons for {movement}.\", style='Heading 2')\n",
    "\n",
    "        if movement == 'CMJ':\n",
    "            # Fetch CMJ data for the client\n",
    "            client_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg,\n",
    "                       Force_at_PP, Vel_at_PP\n",
    "                FROM CMJ WHERE name = ?\n",
    "            \"\"\", (client_name,))\n",
    "            client_cmj_data = client_cursor.fetchone()\n",
    "            reference_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg,\n",
    "                       Force_at_PP, Vel_at_PP\n",
    "                FROM CMJ\n",
    "            \"\"\")\n",
    "            reference_cmj_data = np.array(reference_cursor.fetchall())\n",
    "            \n",
    "            # --- CMJ power files & analysis ---\n",
    "            cmj_files = find_power_files(\"CMJ\")\n",
    "            if cmj_files:\n",
    "                cmj_traces = [load_power_txt(pf) for pf in cmj_files]\n",
    "                add_power_analysis_section(\n",
    "                    doc, movement=\"CMJ\", traces=cmj_traces, fs_hz=1000,\n",
    "                    tmpdirname=tmpdirname, reference_cursor=reference_cursor,\n",
    "                    reference_table=\"CMJ\"\n",
    "                )\n",
    "\n",
    "            # Ensure data exists before proceeding\n",
    "            if client_cmj_data and reference_cmj_data.size > 0:\n",
    "                # Generate bar graphs for each variable in CMJ\n",
    "                variables = ['JH_IN', 'PP_FORCEPLATE', 'PP_W_per_kg', 'Force_at_PP', 'Vel_at_PP']\n",
    "                for i, var in enumerate(variables):\n",
    "                    # Format the variable name by removing underscores\n",
    "                    formatted_var = var.replace('_', ' ')\n",
    "                    \n",
    "                    # Add variable title before the graph\n",
    "                    doc.add_paragraph(f\"{formatted_var} Comparison\", style='Heading 2')\n",
    "                    \n",
    "                    # Generate the bar graph and add to document\n",
    "                    bar_image = generate_bar_graph(var, client_cmj_data[i], reference_cmj_data[:, i], f'{formatted_var} Comparison', tmpdirname)\n",
    "                    doc.add_picture(bar_image, width=Inches(6))\n",
    "                \n",
    "                # Generate scatter plot for CMJ (Force_Peak_Power vs. Velo_Peak_Power)\n",
    "                client_cmj_dict = {'Force_at_PP': client_cmj_data[3],\n",
    "                   'Vel_at_PP':   client_cmj_data[4]}\n",
    "                reference_cmj_dict = pd.DataFrame(reference_cmj_data, columns=variables)\n",
    "                \n",
    "                # Add scatter plot title and image\n",
    "                doc.add_paragraph(\"Force vs. Velocity Scatter Plot\", style='Heading 2')\n",
    "                scatter_image = generate_scatter_plot(client_cmj_dict, reference_cmj_dict,\n",
    "                                                      'Force_at_PP', 'Vel_at_PP',\n",
    "                                                      'CMJ: Force vs. Velocity', tmpdirname)\n",
    "                doc.add_picture(scatter_image, width=Inches(6))\n",
    "    \n",
    "        # ───────────────────────── DJ ─────────────────────────        \n",
    "        elif movement == 'DJ':\n",
    "            client_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg, Force_at_PP, Vel_at_PP, CT, RSI\n",
    "                FROM DJ\n",
    "                WHERE name = ?\n",
    "            \"\"\", (client_name,))\n",
    "            client_dj_data = client_cursor.fetchone()\n",
    "\n",
    "            # ── REFERENCE (robust: pandas + print counts) ─────────────────────\n",
    "            dj_vars = ['JH_IN', 'PP_FORCEPLATE', 'PP_W_per_kg',\n",
    "                       'Force_at_PP', 'Vel_at_PP', 'CT', 'RSI']\n",
    "            ref_sql = f\"SELECT {', '.join(dj_vars)} FROM DJ\"\n",
    "            reference_dj_df = pd.read_sql_query(ref_sql, reference_conn)\n",
    "            print(f\"DJ reference rows: {len(reference_dj_df)}\")\n",
    "            reference_dj_data = reference_dj_df.to_numpy(dtype=float, copy=False)\n",
    "\n",
    "            # ── POWER OVERLAY (files like *DJ*_Power*.txt or DJ_Power*.txt) ───\n",
    "            power_dir = r\"D:\\Athletic Screen 2.0\\Output Files\"\n",
    "            dj_power_files = (sorted(globmod.glob(os.path.join(power_dir, \"*DJ*_Power*.txt\"))) or\n",
    "                  sorted(globmod.glob(os.path.join(power_dir, \"DJ_Power*.txt\"))))\n",
    "            # --- DJ power files & analysis ---\n",
    "            dj_files = find_power_files(\"DJ\")\n",
    "            if dj_files:\n",
    "                dj_traces = [load_power_txt(pf) for pf in dj_files]\n",
    "                add_power_analysis_section(\n",
    "                    doc, movement=\"DJ\", traces=dj_traces, fs_hz=1000,\n",
    "                    tmpdirname=tmpdirname, reference_cursor=reference_cursor,\n",
    "                    reference_table=\"DJ\"\n",
    "                )\n",
    "\n",
    "            # ── BAR GRAPHS + SCATTER (with reference) ─────────────────────────\n",
    "            if client_dj_data:\n",
    "                for i, var in enumerate(dj_vars):\n",
    "                    doc.add_paragraph(f\"{var.replace('_',' ')} Comparison\", style='Heading 2')\n",
    "                    ref_col = reference_dj_data[:, i] if len(reference_dj_df) else np.array([])\n",
    "                    bar = generate_bar_graph(var, client_dj_data[i], ref_col,\n",
    "                                             f'{var} Comparison', tmpdirname)\n",
    "                    doc.add_picture(bar, width=Inches(6))\n",
    "\n",
    "                client_dj_dict = {'Force_at_PP': client_dj_data[3],\n",
    "                                  'Vel_at_PP'  : client_dj_data[4]}\n",
    "                doc.add_paragraph(\"Force vs. Velocity Scatter Plot\", style='Heading 2')\n",
    "                dj_scatter = generate_scatter_plot(\n",
    "                    client_dj_dict,\n",
    "                    reference_dj_df if not reference_dj_df.empty else pd.DataFrame(columns=dj_vars),\n",
    "                    'Force_at_PP', 'Vel_at_PP',\n",
    "                    'DJ: Force vs. Velocity', tmpdirname\n",
    "                )\n",
    "                doc.add_picture(dj_scatter, width=Inches(6))\n",
    "            else:\n",
    "                print(\"No DJ client row found.\")\n",
    "                \n",
    "        # ───────────────────────── PPU ───────────────────────   \n",
    "        elif movement == 'PPU':\n",
    "            # Fetch PPU data for the client\n",
    "            client_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg,\n",
    "                       Force_at_PP, Vel_at_PP\n",
    "                FROM PPU WHERE name = ?\n",
    "            \"\"\", (client_name,))\n",
    "            client_ppu_data = client_cursor.fetchone()\n",
    "\n",
    "            # Reference pull\n",
    "            reference_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg,\n",
    "                       Force_at_PP, Vel_at_PP\n",
    "                FROM PPU\n",
    "            \"\"\")\n",
    "            reference_ppu_data = np.array(reference_cursor.fetchall(), dtype=float)\n",
    "\n",
    "            # --- PPU power files & analysis (optional) ---\n",
    "            ppu_files = find_power_files(\"PPU\")\n",
    "            if ppu_files:\n",
    "                ppu_traces = [load_power_txt(pf) for pf in ppu_files]\n",
    "                add_power_analysis_section(\n",
    "                    doc, movement=\"PPU\", traces=ppu_traces, fs_hz=1000,\n",
    "                    tmpdirname=tmpdirname, reference_cursor=reference_cursor,\n",
    "                    reference_table=\"PPU\"\n",
    "                )\n",
    "\n",
    "            # Ensure data exists before proceeding\n",
    "            if client_ppu_data is not None and reference_ppu_data.size > 0:\n",
    "                variables = ['JH_IN', 'PP_FORCEPLATE', 'PP_W_per_kg', 'Force_at_PP', 'Vel_at_PP']\n",
    "\n",
    "                # Percentile histograms (same as CMJ)\n",
    "                for i, var in enumerate(variables):\n",
    "                    formatted_var = var.replace('_', ' ')\n",
    "                    doc.add_paragraph(f\"{formatted_var} Comparison\", style='Heading 2')\n",
    "                    bar_image = generate_bar_graph(\n",
    "                        var,\n",
    "                        float(client_ppu_data[i]),\n",
    "                        reference_ppu_data[:, i],\n",
    "                        f'{formatted_var} Comparison',\n",
    "                        tmpdirname\n",
    "                    )\n",
    "                    doc.add_picture(bar_image, width=Inches(6))\n",
    "\n",
    "                # Force–Velocity scatter (same as CMJ)\n",
    "                client_ppu_dict = {\n",
    "                    'Force_at_PP': float(client_ppu_data[3]),\n",
    "                    'Vel_at_PP'  : float(client_ppu_data[4]),\n",
    "                }\n",
    "                reference_ppu_df = pd.DataFrame(reference_ppu_data, columns=variables)\n",
    "                doc.add_paragraph(\"Force vs. Velocity Scatter Plot\", style='Heading 2')\n",
    "                ppu_scatter = generate_scatter_plot(\n",
    "                    client_ppu_dict, reference_ppu_df,\n",
    "                    'Force_at_PP', 'Vel_at_PP',\n",
    "                    'PPU: Force vs. Velocity', tmpdirname\n",
    "                )\n",
    "                doc.add_picture(ppu_scatter, width=Inches(6))\n",
    "            else:\n",
    "                print(\"PPU: missing client or reference data; skipping figure set.\")\n",
    "\n",
    "        # ───────────────────────── SLV ───────────────────────        \n",
    "        elif movement == 'SLV':\n",
    "            # ── CLIENT ROWS (Left/Right) ───────────────────────────────────────\n",
    "            client_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg, Force_at_PP, Vel_at_PP\n",
    "                FROM SLV WHERE name = ? AND side = 'Left'\n",
    "            \"\"\", (client_name,))\n",
    "            client_slvl_data = client_cursor.fetchone()\n",
    "\n",
    "            client_cursor.execute(\"\"\"\n",
    "                SELECT JH_IN, PP_FORCEPLATE, PP_W_per_kg, Force_at_PP, Vel_at_PP\n",
    "                FROM SLV WHERE name = ? AND side = 'Right'\n",
    "            \"\"\", (client_name,))\n",
    "            client_slvr_data = client_cursor.fetchone()\n",
    "\n",
    "            # ── REFERENCE (keep side to filter/inspect if needed) ─────────────\n",
    "            slv_vars_no_side = ['JH_IN', 'PP_FORCEPLATE', 'PP_W_per_kg', 'Force_at_PP', 'Vel_at_PP']\n",
    "            ref_sql = f\"SELECT side, {', '.join(slv_vars_no_side)} FROM SLV\"\n",
    "            reference_slv_df = pd.read_sql_query(ref_sql, reference_conn)\n",
    "            print(f\"SLV reference rows: {len(reference_slv_df)}\")\n",
    "\n",
    "            # ── POWER OVERLAYS (Left & Right) ─────────────────────────────────\n",
    "            # We try common patterns. Adjust to your actual export names if needed.\n",
    "            power_dir = r\"D:\\Athletic Screen 2.0\\Output Files\"\n",
    "\n",
    "            # Left trials\n",
    "            slv_left_files = (sorted(globmod.glob(os.path.join(power_dir, \"*SLVL*_Power*.txt\"))) or\n",
    "                  sorted(globmod.glob(os.path.join(power_dir, \"SLV_Power_Left*.txt\"))) or\n",
    "                  sorted(globmod.glob(os.path.join(power_dir, \"SLV_Left*_Power*.txt\"))))\n",
    "\n",
    "            # Right trials\n",
    "            slv_right_files = (sorted(globmod.glob(os.path.join(power_dir, \"*SLVR*_Power*.txt\"))) or\n",
    "                   sorted(globmod.glob(os.path.join(power_dir, \"SLV_Power_Right*.txt\"))) or\n",
    "                   sorted(globmod.glob(os.path.join(power_dir, \"SLV_Right*_Power*.txt\"))))\n",
    "\n",
    "            # --- SLV power files & analysis ---\n",
    "            slv_files = find_power_files(\"SLV\")\n",
    "            if slv_files:\n",
    "                slv_traces = [load_power_txt(pf) for pf in slv_files]\n",
    "                add_power_analysis_section(\n",
    "                    doc, movement=\"SLV\", traces=slv_traces, fs_hz=1000,\n",
    "                    tmpdirname=tmpdirname, reference_cursor=reference_cursor,\n",
    "                    reference_table=\"SLV\"\n",
    "                )\n",
    "\n",
    "            # ── HISTOGRAMS + SCATTER (with reference) ─────────────────────────\n",
    "            if client_slvl_data and client_slvr_data and not reference_slv_df.empty:\n",
    "                # Build name→value dicts so we don't rely on fragile positional indexes\n",
    "                slv_cols = ['JH_IN', 'PP_FORCEPLATE', 'PP_W_per_kg', 'Force_at_PP', 'Vel_at_PP']\n",
    "                client_slvl = dict(zip(slv_cols, map(float, client_slvl_data)))\n",
    "                client_slvr = dict(zip(slv_cols, map(float, client_slvr_data)))\n",
    "            \n",
    "                # We want JH, PP, Force@PP (fixed), and Vel@PP\n",
    "                for var in ['JH_IN', 'PP_FORCEPLATE', 'Force_at_PP', 'Vel_at_PP']:\n",
    "                    doc.add_paragraph(f\"{var.replace('_',' ')} Comparison (Left vs Right)\",\n",
    "                                      style='Heading 2')\n",
    "            \n",
    "                    # Reference distribution by column name\n",
    "                    ref_col = reference_slv_df[var].to_numpy(dtype=float, copy=False)\n",
    "            \n",
    "                    # Client values by column name (correct variables now)\n",
    "                    left_val  = client_slvl.get(var, np.nan)\n",
    "                    right_val = client_slvr.get(var, np.nan)\n",
    "            \n",
    "                    hist = generate_slv_histogram(\n",
    "                        var,\n",
    "                        left_val,            # Left\n",
    "                        right_val,           # Right\n",
    "                        ref_col,\n",
    "                        f'{var} Comparison',\n",
    "                        tmpdirname\n",
    "                    )\n",
    "                    doc.add_picture(hist, width=Inches(6))\n",
    "            \n",
    "                # Scatter Force vs Velocity (merge both sides)\n",
    "                client_slv_all = np.array([list(client_slvl.values()),\n",
    "                                           list(client_slvr.values())], dtype=float)\n",
    "                client_slv_dict = {\n",
    "                    'Force_at_PP': [client_slvl['Force_at_PP'], client_slvr['Force_at_PP']],\n",
    "                    'Vel_at_PP'  : [client_slvl['Vel_at_PP'],   client_slvr['Vel_at_PP']],\n",
    "                }\n",
    "                ref_for_scatter = reference_slv_df[['Force_at_PP', 'Vel_at_PP']]\n",
    "                doc.add_paragraph(\"Force vs. Velocity Scatter Plot\", style='Heading 2')\n",
    "                slv_scatter = generate_scatter_plot(\n",
    "                    client_slv_dict,\n",
    "                    ref_for_scatter,\n",
    "                    'Force_at_PP', 'Vel_at_PP',\n",
    "                    'SLV: Force vs. Velocity', tmpdirname\n",
    "                )\n",
    "                doc.add_picture(slv_scatter, width=Inches(6))\n",
    "            else:\n",
    "                if not (client_slvl_data and client_slvr_data):\n",
    "                    print(\"⚠️  Missing SLV client data – skipping SLV graphs.\")\n",
    "                if reference_slv_df.empty:\n",
    "                    print(\"⚠️  SLV reference pull returned 0 rows.\")\n",
    "    \n",
    "        # elif movement == 'NMT':\n",
    "        #     # Fetch NMT data for the client (10s taps only)\n",
    "        #     print(\"Entering NMT section…\")\n",
    "        #     client_cursor.execute(\"SELECT NUM_TAPS_10s FROM NMT WHERE name = ?\", (client_name,))\n",
    "        #     client_nmt_data = client_cursor.fetchone()\n",
    "        # \n",
    "        #     reference_cursor.execute(\"SELECT NUM_TAPS_10s FROM NMT\")\n",
    "        #     reference_nmt_data = np.array(reference_cursor.fetchall(), dtype=float)\n",
    "        # \n",
    "        #     print(f\"NMT client row present: {client_nmt_data is not None}, reference rows: {len(reference_nmt_data)}\")\n",
    "        # \n",
    "        #     if client_nmt_data and reference_nmt_data.size > 0:\n",
    "        #         nmt_var_label = 'NUM TAPS (10s)'\n",
    "        #         doc.add_paragraph(f\"{nmt_var_label} Comparison\", style='Heading 2')\n",
    "        # \n",
    "        #         # Pass table_hint=\"NMT\" to avoid mis-inference\n",
    "        #         nmt_image = generate_bar_graph(\n",
    "        #             'NUM_TAPS_10s',\n",
    "        #             float(client_nmt_data[0]),\n",
    "        #             reference_nmt_data[:, 0],\n",
    "        #             f'{nmt_var_label} Comparison',\n",
    "        #             tmpdirname          # ← stop here\n",
    "        #         )\n",
    "        #         doc.add_picture(nmt_image, width=Inches(6))\n",
    "        #     else:\n",
    "        #         print(\"NMT: missing client or reference data; skipping figure.\")\n",
    "\n",
    "\n",
    "# Function to convert DOCX to images\n",
    "def docx_to_images(docx_path, output_dir):\n",
    "    # Extract text from the DOCX file\n",
    "    text = docx2txt.process(docx_path)\n",
    "    \n",
    "    # Split the text into lines\n",
    "    lines = text.splitlines()\n",
    "\n",
    "    # Create a blank image with white background\n",
    "    img_width, img_height = 1000, 1500\n",
    "    image = Image.new('RGB', (img_width, img_height), color='white')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Use a simple font\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 20)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Draw the text onto the image\n",
    "    padding = 20\n",
    "    y_text = padding\n",
    "    for line in lines:\n",
    "        if y_text + padding > img_height:\n",
    "            # Save the image and start a new one if the text exceeds the page height\n",
    "            img_path = os.path.join(output_dir, f\"page_{int(y_text / img_height)}.png\")\n",
    "            image.save(img_path)\n",
    "            y_text = padding\n",
    "            image = Image.new('RGB', (img_width, img_height), color='white')\n",
    "            draw = ImageDraw.Draw(image)\n",
    "\n",
    "        # Calculate text size and draw it\n",
    "        text_bbox = draw.textbbox((0, 0), line, font=font)\n",
    "        text_width = text_bbox[2] - text_bbox[0]\n",
    "        text_height = text_bbox[3] - text_bbox[1]\n",
    "\n",
    "        draw.text((padding, y_text), line, font=font, fill=\"black\")\n",
    "        y_text += text_height + padding\n",
    "\n",
    "    # Save the last image\n",
    "    img_path = os.path.join(output_dir, \"final_page.png\")\n",
    "    image.save(img_path)\n",
    "\n",
    "    return img_path\n",
    "\n",
    "doc.save(output_filename)          # ← only one final save\n",
    "print(f\"Document saved at: {output_filename}\")\n",
    "\n",
    "# Close connections\n",
    "client_conn.close()\n",
    "reference_conn.close()\n",
    "\n",
    "# Example usage\n",
    "img_output_directory = r'G:\\My Drive\\Athletic Screen 2.0 Reports\\College Reports\\Images'\n",
    "os.makedirs(img_output_directory, exist_ok=True)\n",
    "\n",
    "# Convert DOCX to images\n",
    "img_path = docx_to_images(output_filename, img_output_directory)\n",
    "print(f\"Images saved at {img_path}\")"
   ],
   "id": "a846af9893379825",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Databases opened successfully.\n",
      "Client Name: Jordan Driver\n",
      "DJ reference rows: 143\n",
      "SLV reference rows: 202\n",
      "Document saved at: G:\\My Drive\\Athletic Screen 2.0 Reports\\College Reports\\Athletic_Report_Jordan Driver_2025-11-07_All.docx\n",
      "Images saved at G:\\My Drive\\Athletic Screen 2.0 Reports\\College Reports\\Images\\final_page.png\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T21:07:57.371055Z",
     "start_time": "2025-11-07T21:07:57.058425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sqlite3\n",
    "import time\n",
    "import os\n",
    "\n",
    "# --- Paths ---\n",
    "source_db_path  = r\"D:/Athletic Screen 2.0/Output Files/movement_database_v2.db\"\n",
    "output_folder   = r\"D:/Athletic Screen 2.0/Output Files/\"\n",
    "target_databases = [\"Athletic_Screen_College_data_v2.db\"]\n",
    "all_data_db_path = os.path.join(output_folder, \"Athletic_Screen_All_data_v2.db\")\n",
    "\n",
    "# --- Retry helper (for \"database is locked\") ---\n",
    "def retry_execute(func, retries=5, delay=1.0):\n",
    "    while retries > 0:\n",
    "        try:\n",
    "            func()\n",
    "            return\n",
    "        except sqlite3.OperationalError as e:\n",
    "            if \"database is locked\" in str(e).lower():\n",
    "                print(\"Database is locked, retrying...\")\n",
    "                time.sleep(delay)\n",
    "                retries -= 1\n",
    "            else:\n",
    "                raise\n",
    "    raise Exception(\"Max retries reached. Database is still locked.\")\n",
    "\n",
    "# --- Open connections ---\n",
    "source_conn = sqlite3.connect(source_db_path, timeout=10)\n",
    "source_conn.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "source_conn.execute(\"PRAGMA busy_timeout=5000;\")\n",
    "source_cursor = source_conn.cursor()\n",
    "\n",
    "target_conns = {\n",
    "    db_name: sqlite3.connect(os.path.join(output_folder, db_name), timeout=10)\n",
    "    for db_name in target_databases\n",
    "}\n",
    "# also keep one combined \"all\" db\n",
    "target_conns[\"all\"] = sqlite3.connect(all_data_db_path, timeout=10)\n",
    "\n",
    "# --- Make targets tolerant + fast ---\n",
    "def prep_conn(conn):\n",
    "    conn.execute(\"PRAGMA journal_mode=WAL;\")\n",
    "    conn.execute(\"PRAGMA busy_timeout=5000;\")\n",
    "\n",
    "for conn in target_conns.values():\n",
    "    prep_conn(conn)\n",
    "\n",
    "# --- Unique keys per table for idempotent UPSERTs ---\n",
    "unique_keys = {\n",
    "    \"CMJ\": [\"name\", \"date\", \"trial_name\"],\n",
    "    \"DJ\" : [\"name\", \"date\", \"trial_name\"],\n",
    "    \"PPU\": [\"name\", \"date\", \"trial_name\"],\n",
    "    \"SLV\": [\"name\", \"date\", \"trial_name\", \"side\"],\n",
    "    \"NMT\": [\"name\", \"date\", \"trial_name\"],\n",
    "}\n",
    "\n",
    "def table_exists(conn, table):\n",
    "    try:\n",
    "        conn.execute(f\"SELECT 1 FROM {table} LIMIT 1\")\n",
    "        return True\n",
    "    except sqlite3.OperationalError:\n",
    "        return False\n",
    "\n",
    "def get_columns(conn, table):\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"PRAGMA table_info({table})\")\n",
    "    # returns list of (name, type)\n",
    "    return [(r[1], r[2]) for r in cur.fetchall()]\n",
    "\n",
    "def sync_table_schema_from_source(table_name):\n",
    "    # read source columns/types\n",
    "    src_cols = get_columns(source_conn, table_name)  # [(name, type), ...]\n",
    "    if not src_cols:\n",
    "        print(f\"Source table {table_name} has no columns; skipping schema sync.\")\n",
    "        return\n",
    "\n",
    "    for db_name, conn in target_conns.items():\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        # Create table if missing (mirror source columns except autoincrement id)\n",
    "        if not table_exists(conn, table_name):\n",
    "            cols_sql = \", \".join(\n",
    "                f\"{col} {ctype or 'TEXT'}\"\n",
    "                for col, ctype in src_cols\n",
    "                if col != \"id\"\n",
    "            )\n",
    "            cur.execute(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    {cols_sql}\n",
    "                )\n",
    "            \"\"\")\n",
    "            conn.commit()\n",
    "\n",
    "        # Add any missing columns (this is how new power metrics get propagated)\n",
    "        tgt_cols = dict(get_columns(conn, table_name))\n",
    "        for col, ctype in src_cols:\n",
    "            if col == \"id\":\n",
    "                continue\n",
    "            if col not in tgt_cols:\n",
    "                cur.execute(f\"ALTER TABLE {table_name} ADD COLUMN {col} {ctype or 'REAL'}\")\n",
    "        conn.commit()\n",
    "\n",
    "        # Ensure unique index for upsert\n",
    "        if table_name in unique_keys:\n",
    "            # 1) dedupe existing rows so the unique index can be created\n",
    "            dedupe_on_keys(conn, table_name, unique_keys[table_name])\n",
    "        \n",
    "            # 2) create the unique index (will succeed now)\n",
    "            idx_cols = \",\".join(unique_keys[table_name])\n",
    "            cur.execute(\n",
    "                f\"CREATE UNIQUE INDEX IF NOT EXISTS idx_{table_name.lower()}_uniq \"\n",
    "                f\"ON {table_name}({idx_cols})\"\n",
    "            )\n",
    "            conn.commit()\n",
    "\n",
    "            \n",
    "def dedupe_on_keys(conn, table, key_cols):\n",
    "    \"\"\"\n",
    "    Remove duplicate rows keeping the latest (highest rowid) per unique key.\n",
    "    key_cols: list like [\"name\",\"date\",\"trial_name\"] (add \"side\" for SLV).\n",
    "    \"\"\"\n",
    "    cols = \", \".join(key_cols)\n",
    "    sql = f\"\"\"\n",
    "        DELETE FROM {table}\n",
    "        WHERE rowid NOT IN (\n",
    "            SELECT MAX(rowid)\n",
    "            FROM {table}\n",
    "            GROUP BY {cols}\n",
    "        )\n",
    "    \"\"\"\n",
    "    conn.execute(sql)\n",
    "    conn.commit()\n",
    "\n",
    "def copy_table_data(table_name):\n",
    "    # Pull from source\n",
    "    try:\n",
    "        source_cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "    except sqlite3.OperationalError as e:\n",
    "        print(f\"Source table {table_name} not found: {e}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    rows = source_cursor.fetchall()\n",
    "    if not rows:\n",
    "        print(f\"No rows to copy for {table_name}. Skipping.\")\n",
    "        return\n",
    "\n",
    "    src_cols_all = [d[0] for d in source_cursor.description]  # includes 'id'\n",
    "\n",
    "    # Make sure targets have ALL columns from source (metrics included)\n",
    "    sync_table_schema_from_source(table_name)\n",
    "\n",
    "    # For each target, compute intersection of columns and UPSERT\n",
    "    for db_name, conn in target_conns.items():\n",
    "        cur = conn.cursor()\n",
    "        tgt_cols_all = [c for c, _ in get_columns(conn, table_name)]\n",
    "        common_cols = [c for c in src_cols_all if c != \"id\" and c in tgt_cols_all]\n",
    "        if not common_cols:\n",
    "            print(f\"{db_name}: No common columns for {table_name}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        placeholders = \", \".join([\"?\"] * len(common_cols))\n",
    "        col_list = \", \".join(common_cols)\n",
    "\n",
    "        if table_name in unique_keys:\n",
    "            uk = unique_keys[table_name]\n",
    "            # Update all non-UK columns on conflict (so metrics refresh)\n",
    "            set_cols = [c for c in common_cols if c not in uk]\n",
    "            set_clause = \", \".join([f\"{c}=excluded.{c}\" for c in set_cols]) or \"\"\n",
    "            on_conflict = f\"ON CONFLICT({', '.join(uk)}) DO \" + (f\"UPDATE SET {set_clause}\" if set_clause else \"NOTHING\")\n",
    "        else:\n",
    "            on_conflict = \"\"\n",
    "\n",
    "        sql = f\"INSERT INTO {table_name} ({col_list}) VALUES ({placeholders}) {on_conflict}\"\n",
    "\n",
    "        # align row values to the common column order\n",
    "        idx_map = {c: src_cols_all.index(c) for c in common_cols}\n",
    "        data = [tuple(row[idx_map[c]] for c in common_cols) for row in rows]\n",
    "\n",
    "        retry_execute(lambda: cur.executemany(sql, data))\n",
    "        conn.commit()\n",
    "        print(f\"Copied/Upserted {len(data)} rows to {table_name} in {db_name}\")\n",
    "\n",
    "# --- Tables to replicate ---\n",
    "tables_to_copy = [\"CMJ\", \"DJ\", \"PPU\", \"SLV\", \"NMT\"]\n",
    "\n",
    "# --- Run the copy ---\n",
    "for table in tables_to_copy:\n",
    "    copy_table_data(table)\n",
    "\n",
    "# --- Close connections ---\n",
    "source_conn.close()\n",
    "for conn in target_conns.values():\n",
    "    conn.close()\n",
    "\n",
    "print(\"Data successfully copied to each target and combined database.\")\n",
    "\n",
    "# --- Optional: clear ASCII .txt exports after ingestion ---\n",
    "ascii_folder = r\"D:/Athletic Screen 2.0/Output Files/\"\n",
    "for filename in os.listdir(ascii_folder):\n",
    "    if filename.lower().endswith(\".txt\"):\n",
    "        file_path = os.path.join(ascii_folder, filename)\n",
    "        try:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {file_path}: {e}\")\n",
    "\n",
    "print(\"All ASCII .txt files cleared after ingestion.\")\n"
   ],
   "id": "acbe8b04e0392085",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied/Upserted 3 rows to CMJ in Athletic_Screen_College_data_v2.db\n",
      "Copied/Upserted 3 rows to CMJ in all\n",
      "Copied/Upserted 3 rows to DJ in Athletic_Screen_College_data_v2.db\n",
      "Copied/Upserted 3 rows to DJ in all\n",
      "Copied/Upserted 2 rows to PPU in Athletic_Screen_College_data_v2.db\n",
      "Copied/Upserted 2 rows to PPU in all\n",
      "Copied/Upserted 4 rows to SLV in Athletic_Screen_College_data_v2.db\n",
      "Copied/Upserted 4 rows to SLV in all\n",
      "No rows to copy for NMT. Skipping.\n",
      "Data successfully copied to each target and combined database.\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/CMJ1.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/CMJ1_Power.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/CMJ2.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/CMJ2_Power.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/CMJ3.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/CMJ3_Power.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/DJ1.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/DJ1_Power.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/DJ2.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/DJ2_Power.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/DJ3.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/DJ3_Power.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/SLVL1.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/SLVL1_Power.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/SLVL2.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/SLVL2_Power.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/SLVR1.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/SLVR1_Power.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/SLVR2.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/SLVR2_Power.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/PPU1.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/PPU1_Power.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/PPU2.txt\n",
      "Deleted: D:/Athletic Screen 2.0/Output Files/PPU2_Power.txt\n",
      "All ASCII .txt files cleared after ingestion.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T21:07:57.435508Z",
     "start_time": "2025-11-07T21:07:57.373066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "\n",
    "# Paths\n",
    "db_source_paths = [\n",
    "    r\"D:/Athletic Screen 2.0/Output Files/movement_database_v2.db\",\n",
    "    r\"D:/Athletic Screen 2.0/Output Files/Athletic_Screen_All_data_v2.db\",\n",
    "    r\"D:/Athletic Screen 2.0/Output Files/Athletic_Screen_College_data_v2.db\"\n",
    "]\n",
    "destination_folder = r\"G:/My Drive/Data/Athletic Screen Data\"\n",
    "\n",
    "# Copy each DB file to the destination\n",
    "for db_path in db_source_paths:\n",
    "    try:\n",
    "        shutil.copy(db_path, destination_folder)\n",
    "        print(f\"Copied {db_path} → {destination_folder}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to copy {db_path}: {e}\")\n"
   ],
   "id": "714e986e51635beb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied D:/Athletic Screen 2.0/Output Files/movement_database_v2.db → G:/My Drive/Data/Athletic Screen Data\n",
      "Copied D:/Athletic Screen 2.0/Output Files/Athletic_Screen_All_data_v2.db → G:/My Drive/Data/Athletic Screen Data\n",
      "Copied D:/Athletic Screen 2.0/Output Files/Athletic_Screen_College_data_v2.db → G:/My Drive/Data/Athletic Screen Data\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T21:07:57.441081Z",
     "start_time": "2025-11-07T21:07:57.436657Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8ffea23ec1497bb6",
   "outputs": [],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
